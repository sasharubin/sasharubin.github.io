This is a review of manuscript number ARTINT-D-16-00179. All numbered
references, e.g., [10], refer to the bibliography of the manuscript. Additional
references are listed below. 

CONTEXT

The main objects of study are (finite-state) fully-observable non-deterministic
planning problems (FOND) and a particular solution concept, i.e., strong-cyclic
policies.  These are policies that satisfy the branching property AG(EF(goal)),
or a variation in which the EF(goal) obligation is dropped once goal has been
reached. 

Such solution concepts are interesting because they capture iterative
trial-and-error strategies such as "pick up a block until succeed" that might
loop forever if a certain effect ("succeed") is not guaranteed [10]. 

Consequently, as the authors point out, the FOND literature assumes that
policies be run in "fair" environments, and that this assumption had not been
sufficiently formalised.

The purpose of this paper is to formalise this assumption, and leverage it to
show that one can reduce FOND with strong-cyclic policies to reactive synthesis
on fair environments, and achieve the known optimal complexity, i.e., 1EXPTIME.

The connection between planning and synthesis is known to be a tight one.  FOND
planning is nothing else than synthesising winning strategies in 2-player
graph-games of perfect information.  However, there are two important
distinctions between the formalisms.
- Planning typically induces an exponentially large graph (i.e., n variables
  induce 2^n states), and thus the complexities are typically one exponent
  larger (e.g., reachability games are PTIME-complete, while strong FOND
  planning is EXPTIME-complete).
- The manuscript focuses on non-randomised deterministic policies, which
  correspond to deterministic memoryless strategies. In many games such
  strategies suffice, i.e., if there is no deterministic memoryless strategy
  achieving an objective, then there is no strategy at all.

CONTRIBUTION

The authors define a linear-temporal property over executions called
"state-strong fairness" that says that if the same action is made in the same
state infinitely often, then every possible effect also appears infinitely
often. 

State-strong fair executions (of a given policy) have the following fundamental
property: if a state s is seen infinitely often then every state s' reachable
(using the policy) from s is also seen infinitely often. 

They prove that a policy is strong-cyclic if and only if every state-strong fair
execution of the policy reaches the goal (Theorems 1 and 2).  Informally, the
reason this is true is the following:
- only if: if a state s is seen infinitely often then, by state-strong fairness,
  every state s' reachable from s (in P using the policy) is also seen
  infinitely often; so by strong-cyclic, at least one of these states satisfies
  the goal.
- if: if a state s is reachable (in P using the policy) and the goal is not
  reachable from s, then no execution from s (in particular the state-strong
  fair ones) reaches the goal, contradicting the assumption.

The authors show that another natural fairness property does not capture
strong-cyclic policies, i.e., "if an action is executed infinitely many times,
every non-deterministic outcome will occur infinitely many times".

They then provide a reduction from finding strong-cyclic policies in FOND
problems to solving Buchi-games, i.e., graph-games with repeated reachability
objectives. The authors state that the reduction is inspired by one from the
stochastic games literature [5].

EVALUATION

I unreservedly commend the authors for their goal of identifying and cleaning-up
loose and misleading assumptions from the literature. Too much time is wasted on
extending ideas with poor foundations. I especially appreciate the statements of
Theorems 1 and 2.

However, I have serious concerns about the novelty of the technical details.  

In short, Theorems 1 and 2 follow from standard properties of MDPs; and, the
fact that there is a natural reduction of strong-cyclic FOND to game-solving
follows from important connections between fix-point algorithms and parity
games.

Here are my justifications.

State-strong fairness is a well-known notion in the planning and verification
communities.

For instance, it was formally defined in [34, Section 2.1] where it is shown how
to reduce planning for LTL objectives under state-strong fairness to
strong-cyclic planning. The exact relationship between the manuscript and [34]
is not clearly established (see OTHER COMMENTS).

Let me focus on the setting in which state-strong fairness is fundamental, i.e.,
the probabilistic setting, and in particular Markov Decision Processes with the
solution concept "the goal is reached with probability 1". 

The connection with probability is implicit in the manuscript, e.g., Example 1
gives a FOND representation for the problem of flipping coins until heads
appear, and the stated inspiration for the reduction is a paper on stochastic
parity games [5].

However, much more is known. A policy on an MDP induces a Markov Chain. The
set of executions in a Markov Chain that are state-strong fair has measure
equal to one. Thus: 

FACT 1. A policy ensures the goal with probability 1 if and only if it ensures
the goal on state-strong executions. 

What is the connection with strong-cyclic policies? It turns out that one can
reduce the problem "does there exist a strategy that achieves the goal with
probability 1" to the problem "does there exists a strategy that induces a
Markov chain whose underlying graph satisfies AG(EF(goal))", see for instance
[FKNP11, Algorithm 4]. In other words, the algorithm reduces the problem to
checking if there exists a strong cyclic strategy. So, given a policy, apply the
algorithm to the induced Markov chain of that policy, and get:

FACT 2. A policy ensures the goal with probability 1 if and only if it is
strong-cyclic.

Putting FACTS 1 and 2 together, we get Theorems 1 and 2.

Actually, the algorithm in [FKNP11] can be thought of as a parity game.  Here
are some details.  The algorithm calculates the set of states for which there is
a policy ensuring that the goal is reached with probability 1. The algorithm is
a nesting of two fix-points computations, a least inside a greatest, over the
underlying transition-system of the MDP.  This amounts to model-checking a
certain mu-calculus formula over this transition-system.  Moreover, model
checking mu-calculus is equivalent to solving parity games [GTW02].  In fact,
the corresponding parity game has 3 colours, and so can be solved in quadratic
time in the number of states (of the MDP).

Thus there is a natural reduction of strong-cyclic FOND to solving 3 colour
parity games, resulting in an EXPTIME procedure.

OTHER COMMENTS

- There is some confusion about the differences and similarities between reactive
synthesis and game solving.

The authors state that they demonstrate "how efficient approaches to controller
synthesis can be used so solve FOND planning tasks" (pg 2.) 

What they mean, I think, is that they plan to show how to reduce strong-cyclic
FOND to solving games of one kind or another (and achieve optimal complexity
with this reduction). Indeed, they don't reduce to reactive synthesis (as also
the title suggests), but to certain games on graphs. 

Why does this distinction between reactive synthesis and games on graphs matter?
It has ramifications on the complexity of synthesis.

LTL games (i.e., games on graphs with LTL objectives, such as Buchi games) are
distinct from reactive synthesis. The former have an explicit arena while in the
latter the arena is implicit and compactly encoded in the formula. As a result,
the complexity of solving LTL games is 2EXPTIME in the formula and polynomial in
the arena, while the complexity of reactive synthesis is 2EXPTIME in the
formula.

- The title uses the phrase "assumption-based reactive synthesis". An explanation
of "assumption-based" means would be helpful.

- The definitions in the preliminary section (sec 2.1) should be tightened.
 - "condition" is undefined
 - the definition of state, although correct, is clunky.
 - you allow non-deterministic strategies but then focus on deterministic ones for
   most of the paper.
 - the definition of "possible executions" uses the notion of "execution". again,
   although correct, it makes for hard reading.
 - in the definition of "possible executions" it seems that the condition s_
   \models Pre_{o_i} is superfluous since you already stated that o_i \in
   \pi(s_i).
 - are "possible executions" infinite, as suggested by the notation?
 - the definition of "closed" does not match the explanation after it.
 - the definition of "strong cyclic plan" is too informal. 
 - the statement about "strong policies" says that all executions are finite and
   acyclic; doesn't the fact that policies are memoryless mean that finite
   implies acyclic?
 - please define "classical planning".

- Definition 4. "strong-cyclic plan solution". Isn't "solution" superfluous?

- More generally, "plan", "policy" and "solution" are all used in the
  manuscript; are these interchangeable?

- I found the main purpose of the paper obscurely described, until I read the
  theorems. For instance, the abstract states "We study conditions that the
  environments must entail so that the plans guarantee achieving the goals...
  [we] show that strong-cyclic plans are correct solution concepts for fair
  environments", and later (pg. 7) "it is ... important to understand and
  formalize the contexts under which these type of plans will indeed achieve the
  objectives".  The notions "correct solution concept" and "contexts" are too
  vague to be useful.

- The discussion in the related work about the current state of the literature
  regarding the meaning of "fair environment" would be a most valuable addition.
  E.g., what is the exact relationship between the present work and [34]. There
  they show how to reduce state-strong fair realisability to strong-cyclic
  planning. Although it says (pg. 10) "The theorem is related to Theorem 5 in
  [34], though ours crystallize the fairness assumption explicitly within
  Traverso et al.’s FOND planning logical foundational framework (see Section 5
  for further discussion).", I found neither this statement nor the discussion
  in Section 5 clear.

- The citation in "While feasible, synthesis under fairness assumptions is
  computationally very demanding (Vardi [46]...)" is not appropriate. That paper
  covers a much more general setting, i.e., LTL objectives and imperfect
  information.

- Pg 24. CTL synthesis is mentioned. This seems out of context.

REFERENCES

[FKNP11] V. Forejt, M.Z. Kwiatkowska, G. Norman, D. Parker.  Automated
Verification Techniques for Probabilistic Systems. SFM 11, 53-113, 2011.

[GTW02] Automata, Logics, and Infinite Games:A Guide to Current Research.
E. Grädel, W. Thomas and T. Wilke, Thomas (Eds.), 2002.

[GV15] G. de Giacomo and M. Vardi. Synthesis for LTL and LDL on Finite Traces.
IJCAI 15, 1558-1564, 2015.

REVIEWER

Sasha Rubin (I am not in favour of single-blind reviews).
