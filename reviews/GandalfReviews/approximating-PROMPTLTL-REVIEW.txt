Approximating Optimal Bounds in Prompt-LTL Realizability in Doubly-exponential Time

CONTEXT
-------
PROMPT-LTL is an extension of LTL by a "prompt eventuality" operator that says 
that there is some bound k such that all eventualities (in all paths) are fulfilled 
within at most k steps [2,13].

This paper studies the realisability problem for PROMPT-LTL. This is like LTL 
realisability (Pnueli/Rosner), but with two differences: PROMPT-LTL instead of 
LTL, and turn-based instead of concurrent.

It is known that if a PROMPT-LTL formula is realisable, it is realisable with a k
of size at most 2EXP (in the size of the formula) and a finite-strate strategy of 
size at most 2EXP. Moreover, computing a minimum k (for a given formula) can be done 
in 3EXPTIME. It is open whether it can be done in 2EXPTIME.

RESULTS AND TECHNIQUES
----------------------
The paper has theoretical and empirical results.

Theoretical.
The first main result (Theorem 2) is that one can, in 2EXPTIME, approximate the 
minimal k by a factor of 2. The authors show that there is a trade-off 
between the prompt bound k and the size n (number of states) of the strategy 
realising the formula. They study the Pareto frontier and supply upper and lower 
bounds on the number and values of Pareto pairs (n,k). 

Empirical.
The authors implement a tool that 2-approximates the minimum bound k for a given PROMPT-LTL 
formula. The tool is tested on a class of formulas representing arbiters. The results
show that there is a factor of 10 slowdown compared with LTL realisability, and suggest there 
is a trade-off, i.e., to decrease k one needs to increase n (and vice versa).

EVALUATION
----------
The topic of the paper should interest the GandALF community because PROMPT-LTL falls under the scope of
formal methods for quantitive properties/systems, which is a hot-topic right now.

I find the main result, i.e., that the minimal k can be approximated within a factor of 2,
to be a very nice result.

Regarding technical depth, the proof of Theorem 2 is almost an immediate consequence of previous work [2]. 
The proofs of the upper bounds regarding the Pareto frontier are straightforward 
applications of [2,19]. The proofs of the lower bounds require some ingenuity.

I found the statements of Lemma 5 and Theorems 3 and 4 in the section about trading 
memory for quality hard to follow. What are the consequences of these results? 
Why do you consider Lemma 5 to have "... presented upper bounds on the number of 
Pareto positions" (page 10)?  How do Theorems 3 and 4 "...exhibit such a Pareto 
frontier." (page 10)? Generally speaking, what is the take-away
message of these Theorems and Lemmas?

A brief discussion of the concurrent (rather than turn-based case) is needed. E.g., why only treat
the turn-based case? what is known about the concurrent case? etc.

COMMENTS
--------
- Theorem 1, and elsewhere, missing |...|

- Sometimes Lemmas are referenced twice, e.g., Lemma 22 instead of Lemma 2.

- Then "approximation ratio" is discussed once (on page 6). Perhaps say what its relevance is.

- In the conclusion, you write that the upper bounds "...reduces the search space of our algorithm". 
Please clarify what this means as there seems to be no mention of the reduction in Section 4.




