The Almost Equivalence by Asymptotic Probabilities for
Regular Languages and Its Computational Complexities

Context
-------
This papers studies a notion of equivalence between regular languages that is based on a natural 
combinatorial/probabilistic notion of the "measure" of a language.

The measure of a language L is the (limit of the) probability that a word of a given length, chosen uniformly at random, 
is in L. Formally, for a fixed alphabet \Sigma, the measure of a language L \subseteq \Sigma^*
is the limit as n tends to infinity of the proportion of words over alphabet \Sigma of length n that are in L.

For instance, over alphabet \Sigma = {a,b}, L = {a}{a,b}* has measure 1/2 and L = {a}* has measure 0, 
and L = (\Sigma \Sigma)*  has no measure. 

Then, two regular languages L and L' are p-equivalent if (dfn) the measure of their symmetric difference is zero.

The motivation for this paper is to study the complexity of deciding p-equivalence between regular languages (for different representations of regular languages), and to compare this with the complexity of testing ordinary equivalence (i.e., language equality).

Results and Techniques
----------------------
Regular languages are represented as DFAs, NFAs, or regular expressions. Both the case of a unary alphabet and a non-unary alphabet are treated. This yields six cases.

The main results give, for all but one of the cases, the exact computational complexity of deciding p-equivalence 
and whether or not a language has measure zero or one. The missing case is a matching lower bound for DFAs over unary alphabets.

The results, conveniently summarised in Table 1, show that the computational complexity of deciding p-equivalence is the 
same as that of deciding ordinary equivalence (i.e., language equality). The table also shows that deciding whether or not a language has measure in the set {0,1} has the same complexity.

The techniques rely on a combinatorial characterisation of zero measure for regular languages (Lemma 1.1): it says that a DFA has non-zero iff some bottom strongly connected component has a final state. This condition can be expressed by a formula of FO+TC. Then, complexity results follow standard reductions and classic results from descriptive complexity (e.g., NL = FO+TC).

The paper also shows that other notions of equivalence (from the literature) agree on the regular sets of measure zero, and thus result in the same notion of equivalence on the regular sets.

Evaluation
----------

Pros.

The definition of p-equivalence seems novel. The paper convincingly argues that it is natural and robust. Proposition 3.2 (which shows that p-equivalence is robust) relies on the usual automata-theoretic arguments (like pumping), deeper results (like Lynch's theorem about convergence laws for regular languages), and standard theorems from the theory of limits and series (e.g., squeeze theorem, Stolz-Cesaro theorem).

The main combinatorial lemma (Lemma 1.1) is another nice result using similar methods (elementary automata-theory and theory of limits and series). By the way, the non-standard English made it unclear if Lemma 1.1 already appears in [22]; it does not.

I think this paper (and [22]) open up a lot of interesting questions, e.g., give a logical or algebraic characterisation of 
those regular languages with measure zero? or with measure one? It would be valuable to have a slightly more detailed discussion of future work.

Cons.

I have two serious concerns. 

First, it seems that a minor variation of the characterisation in [22] can be used in place of Lemma 1.1 in the proofs of the complexity results. Thus, it is not clear why Lemma 1.1 is needed. And, in my opinion, without Lemma 1.1, the paper's technical depth is reduced. Having said that, the proof of Lemma 1.1 seems more direct and to the point than what one could get from [22]; so perhaps this is an advantage.

Second, the discussion of related work is very sparse. Growth functions of regular languages are well studied by, e.g., Shallit, Shur, Bodirsky, Sakarovitch, etc. The relationship between the present paper and previous work should be (at least briefly) discussed. Also, a clear discussion showing the similarities and differences between the present paper and [22] is required since they seem very closely related. Also, the relationship with standard zero-one laws should be clarified (e.g., FOL without order has zero-one laws, while FO+< does not, and this paper is about words, i.e., with <).

Overall, the paper has some depth (Lemma 1.1), is well justified, gives some nice complexity results, and opens up avenues for further work. I recommend acceptence, and if accepted, I hope the author reorganises, and strongly urge that the author removes the ambiguities from the English, and drastically improve the discussion of related work for the proceedings. 

Other comments
--------------

- Less vital, the organisation could be improved. Perhaps the basic decision problem that should be studied first is whether a regular language has measure zero (indeed, this is what Lemma 1.1 actually characterises). Then, one can deduce the complexity results as corollaries.

- A note on terminology. Having a zero-one law should not a property of a language, but a property of a set of languages.  
E.g., the set of FO-definable languages (without order) has the zero-one law, while the set of MSO-definable languages does not. Moreover, it is not clear to me why it is interesting to study the complexity of whether a given regular language has measure in the set {0,1}. Studying the structure of such languages (e.g., algebraically, as is done in [22]) has clearer motivation.

- A short summary of the results used from descriptive complexity should appear somewhere, perhaps in "preliminaries".

- the abstract could be made clearer and more precise. 

- The alphabet is important. E.g., if Sigma = {a} then L=a* has measure 1, but if Sigma = {a,b} then L=a* has measure 0.

- page 1: it is worth mentioning that the limit may not be defined.

- page 2: Theorem 1.2 should be stated more carefully. E.g.,
There is an algorithm that given as input a finite relational signature \sigma and a sentence \phi of FO[\sigma], decides whether \phi is almost surely valid

"One of our main motivation ...": this is not a *motivation* to introduce p-equivalence, but rather a natural problem to ask once it has been introduced.

"This logical characterization is useful to give some algorithms for the p-=equivalence problems by descriptive complexity" is better written, e.g., "This logical characterization, coupled with standard results from descriptive complexity [11], are used to decide the p-equivalence problem for various representations of regular languages".

Stating Lemma 1.1 before giving the notation for DFAs is not elegant. Also, it is worth mentioning that the proof will appear later in the body.

The two paragraphs starting "Second, ..." and ending "complexities" should be made clearer.

- page 3
Here and elsewhere, "let A be a finite set of alphabet" should be "let A be a finite set of letters" or "Let A be a finite alphabet".

DFA: it would be more elegant to give a recursive definition of delta(q,s)

- page 4
Here and elsewhere, "is not followed" should be "does not hold" or "is false" or "is not true".
Similarly, "are obviously followed" should be "obviously hold".

line -3, delete "then".

- page 5
"are considerable" --> "appear in the literature".

missing |...| in (1) and (2).

Proof of prop 3.2, 3 implies 1, first math environment, shouldn't the first inequality \geq be an equality = ?

"for any b" --> "for every b".

last line of proof, shouldn't "\mathfrac{|A|^b}{\Sigma ...}" be "\mathfrac{1}{b}"?

"which is another characterisation from [22]" --> "which is different from the characterisations in [22]".

line -1 of page: missing |...|

- page 6, 

please clarify how the pigeonhole principle is used.

"is easily followed"--> "easily follows"

- page 7
Brzozowski
Meyer or Stockemeyer?

proof, item 3: line -5, \delta should be "R_a"
line -3, Fs should be F_1 and F_2.


- page 8
why do you give a direct algorithm? are there no results from descriptive complexity that you could use? why not?

- page 9

I think it is important to a) define generalised equivalence outside of the corollary, and b) instantiate the definition to see that it is useful.

Proof part 2, say more about what E and the primes are. 
Proof part 3, again, say more. What is [11, Lemma 1.10]?
what is CSL?
"leftest"-->"left" or "leftmost position"








==================================================================================================================================

