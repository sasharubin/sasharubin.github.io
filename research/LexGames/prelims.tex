\section{Preliminaries}



In this section we define our game model: our games are played on finite graphs (rather than games in extensive-form or normal-form), games are multi-player, agents move concurrently (which includes the special case that they move sequentially), and agents play deterministic (rather than randomised) and memoryfull (rather than memoryless or finite-memory) strategies, and agents are trying to maximise their payoffs which are given as a lexicographic combination of a qualitative \LTL formula and a quantitative long-term average of the rewards of its actions.

We fix some notation. If $X$ is a set, then $X^\omega$ is the set of all infinite sequences over $X$. 
If $X$ and $Y$ are sets then $X^Y$ is the set of all functions $f:Y \to X$.

\subsection{Lexicographic Games}
	
		A \emph{weighted arena} is a
		tuple $A=\tpl{Ag,  Act, Ap, W, St, \iota,tr, \lambda,\kappa}$ where
		\begin{enumerate} \item $Ag$ is a finite non-empty set of \emph{agents} (write 
			$N			= \card{Ag}$);
			\item $Act$ is a finite non-empty set of the \emph{actions};
						\item $Ap$ is a finite non-empty set of \emph{atoms};

			\item $W \subset \mathbb{Z}$ is a finite non empty-set of integer \emph{weights}; 
			\item $St$ is a finite non-empty set of \emph{states} and $\iota$
						is the \emph{initial state};
			
			\item $tr: St \times Act^{Ag} \rightarrow St$ is
						a \emph{transition function} mapping each pair, consisting of a
						state and an action for each agent, to a successor state;
			\item $\lambda:St \to 2^{Ap}$ is the \emph{labeling function}; 
			\item $\kappa: Act \to W$ is a \emph{weight function} mapping each action to an integer.
		\end{enumerate}

		

\head{Executions}
An \emph{execution} $\pi = s_0 d_0 s_1 d_1 \cdots$ is an infinite sequence over $St \times Act^{Ag}$ such that $s_0 = \iota$ and 
$tr(s_i,d_i) = s_{i+1}$ for all $i$.
In particular, $d_i(a)$ is the action of agent $a$ in step $i$. Let $\exec$ denote the set of all
executions. Extend $\lambda$ and $\kappa$ to executions: i.e., each execution $\pi = s_0 d_0 s_1 d_1 \cdots $ induces
\begin{enumerate}

\item a sequence $\lambda(\pi) = \lambda(s_0) \lambda(s_1) \cdots$  of labels; and

\item for each agent $a$, a sequence $\kappa_a(\pi) = \kappa(d_0(a)) \kappa(d_1(a)) \cdots$.
\end{enumerate}



\head{Lexicographic games}
% A function of the form $F:W^\omega \to \mathbb{R} \cup \{-\infty, +\infty\}$ is called an \emph{aggregation function}.
For a sequence $\alpha \in \mathbb{R}^\omega$, let $\mp(\alpha)$ be the \emph{mean-payoff} of $\alpha$, i.e., 
\[
 \mp(\alpha) = \lim \inf_{n \to \infty} \frac{1}{n} \Sigma_{j=0}^{n-1} \alpha_j.
\]



% A \emph{qualitative \LTL objective} is a tuple $\Psi = (\varphi_a)_{a \in Ag}$ of \LTL formulas.
A \emph{$\LEX(\LTL,\mp)$ game} is a tuple $G = \tpl{A,\Psi}$ where $A$ is a weighted arena and $\Psi = (\varphi_a)_{a \in Ag}$ is a tuple of \LTL formulas.
The \emph{payoff for agent $a$ of an execution $\pi$} is the pair $pay_a(\pi) = (x,y) \in \{0,1\} \times \mathbb{R}$ where
$x = 1$ iff $\lambda(\pi) \models \varphi$, and $y = \mp(\kappa_a(\pi))$. We define a total ordering on payoffs:
$(x,y) \lex (x',y')$ iff, either $x < x'$, or $x = x'$ and $y < y'$. Each agent is trying to maximise its payoff. In other words, agent $a$'s primary goal is to make $\varphi_a$ true, and its secondary goal is to maximise it's reward $\mp(\kappa_a(\cdot))$. 

\begin{remark}
We consider weights to be rewards to be maximised. One may, instead, consider them costs to be minimised. All our results hold for such cost-games. Indeed, given a weighted arena $A$ with weights $\kappa$, form a weighted arena $A'$ in which all weights are replaced by their negations. 
Then, an agent, say $a$, that maximises its payoff in $A'$ has the primary goal of making $\varphi_a$ true and the secondary goal of maximising $\mp(-\kappa_a(\cdot))$. 
But maximising $\mp(-\kappa_a(\cdot)) = - \mp(\kappa_a(\cdot))$ is the same as minimising $\mp(\kappa_a(\cdot))$.
\end{remark}
 
\head{Strategies and Nash Equilibria}
A \emph{history} is a finite sequence $s_0 d_0 \cdots s_{n-1} d_{n-1} s_n$ such that $s_0 = \iota$ and $tr(s_i,d_i) = s_{i+1}$ for $i < n$.
A \emph{strategy profile} is a function $\sigma: Ag \to (\hist \to Act)$. A strategy profile $\sigma$ induces a unique execution $\pi_\sigma$, i.e., $\pi_\sigma = s_0 s_1 \cdots$ where $s_0 = \iota$ and $s_{i+1} = tr(s_i,d)$ where $d(a) = \sigma(a)(s_0 s_1 \cdots s_i)$ for $i > 0$.
A strategy profile $\sigma$ is a \emph{Nash-Equilibrium} if for every strategy profile $\sigma'$ that disagrees with $\sigma$ on exactly one component, say $\sigma'(a) \neq \sigma(a)$ and $\sigma'(b) = \sigma(b)$ for all $b \neq a$, we have that $pay_a(\pi_{\sigma'})  \lexeq  pay_a(\pi_\sigma)$.

% % \head{Concrete aggregation functions}
% % We will consider various aggregation functions.
% % For $\alpha \in W^\omega$, 
% % % let $occ(\alpha)$ be the values occuring in $\alpha$, and let $inf(\alpha)$ be the values occuring infinitely often in $\alpha$.
% % 
% % \begin{enumerate}
% %  \item $\mp(\alpha) \in \mathbb{R}$ is the mean-payoff of $\alpha$, i.e., the lim-inf of the average of the first $n$ terms of $\alpha$; 
% %  \item $\max(\alpha) \in \mathbb{Z}$ is the maximum value of the weights in $\alpha$.   
% % \end{enumerate}

\head{Turn-based Arenas} Arenas in which every state is ``controlled'' by a single agent are called turn-based (sometimes called sequential). We define such arenas as a special case of concurrent arenas. Formally, 
a weighted weighted arena $A$ is called \emph{turn-based} if for every $s \in St$ there exists an agent $a \in Ag$ such that
$tr(s,d) = tr(s,d')$ for all $d,d' \in Act^Ag$ such that $d(b) = d'(b)$ for all $b \neq a$. A game is called \emph{turn-based} if its arena is turn-based.


\subsection{Decision Problems}

The \emph{NE-language $\NE(G)$} of a game $G$ is the set of strategy-profiles that are Nash-Equilibria. We consider the following decision problems:

\begin{enumerate}
 \item \emph{NE-membership} is the problem of deciding, given $G$ and a finite-state profile $\sigma$, if $\sigma \in \NE(G)$.
 \item \emph{NE-emptiness} is the problem of deciding, given $G$, if $\NE(G)$ is empty.
 \item \emph{E-Nash} is the problem of deciding, given $G$ and an \LTL formula $\varphi$, if there exists $\sigma \in \NE(G)$ such that $\lambda(\sigma) \models \varphi$.
\end{enumerate}

\subsection{Justification of our Model}
In this section we justify lexicographic games and our focus on pure memoryfull strategies.

First, $\LEX(\LTL,\mp)$-games generalise a number of important models, i.e., \LTL-games (let all weights be $0$) and $\mp$-games 
(let all formulas be $\true$). Note that \LTL-games generalise safety and reachability games. \todo{mention more papers, e.g., taxation model}

Second, we focus on pure strategies instead of mixed strategies. This is because $\LEX(\LTL,\mp)$-games need not have mixed NE (this is true even for two-player zero-sum games with reachability objectives~\cite{DBLP:journals/corr/abs-1109-6220}), and it is undecidable whether or not a given game has a mixed NE (this is true even for mean-payoff games~\cite{DBLP:journals/corr/abs-1109-6220}).

Third, we focus on decision problems for memoryfull strategies. This is because pure NE need not exist, e.g., matching pennies is a reachability game with no pure NE. Moreover, even if a pure NE exists, it need not have finite-memory NE. This can be seen by considering the following one-player game with \LTL objective $\always \eventually p$: $Act = \{l,r\}$, $Ap = \{p\}$, $W = \{0,10\}$, $St = \{\iota,v_1,v_2\}$, $tr(\iota,l) = v_1$, $tr(\iota,r) = v_2$, $tr(v_1,x) = tr(v_2,x) = \iota$ for $x \in Act$, $\lambda(v_1) = p$, $\kappa(\iota) = \kappa(v_1) = 10$ and $\kappa(v_2) = 0$. It is shown in \cite[Example $1$]{DBLP:journals/corr/abs-1210-3539} that there is a pure strategy that achieves payoff $(1,10)$, but every finite-state strategy achieves less. 

Finally, we focus on decision problems for concurrent games rather than turn-based games because, as we now show, turn-based games always have NE.

\begin{theorem} \label{thm:TB}
 Turn-based $\LEX(\LTL,\mp)$-games always have NE.
\end{theorem}
\begin{proof}[Sketch]
The proof is in three steps: first, push the weights to the states and replace the \LTL formulas by parity conditions on the states, to get a 
$\LEX(\parity,\mp)$ game with the same set of NE. Second, 
prove that the following games are determined, i.e., they have a value: two-player zero-sum games with a $\LEX(\parity,\mp)$ objective 
(i.e., one player is trying to maximise $pay(\pi)$ while the other is trying to minimise this same payoff). Third, use the existence of optimal strategies (from the second item) to prove the existence of a NE in multiplayer $\LEX(\parity,\mp)$ games. This last step uses the fact that $\LEX(\parity,\mp)$ is prefix-independent, i.e., if player $a$ prefers execution $\pi'$ to $\pi$ then, for all histories $h$, player $a$ also prefers $h \cdot \pi'$ to $h \cdot \pi$ (assuming these are also executions). 

Here are the details. First, push the weights to the states. This is done by replacing $St$ by $St \times Act^{Ag}$ and defining the weight of $(s,d)$ to be $\kappa(d)$. Now translate each \LTL formula $\varphi_i$ into a deterministic parity word automaton  $D_i$. Form the product weighted arena $A' = A \times \prod_i D_i$ where each state is labeled by both the weight from $A$ and a tuple of priorities $\tup{p}$ from the $D_i$s. The payoff for player $i$ is the pair $(x,y) \in \{0,1\} \times \mathbb{R}$ where $x = 1$ iff the smallest priority in co-ordinate $i$ that occurs infinitely often is even, and $y$ is the mean-payoff of the weights. The resulting $\LEX(\parity,\mp)$ game $A$ has the same NE as the original $\LEX(\LTL,\mp)$ game.


Second, let $A_i$ be the two-player game, based on the weighted arena $A$, in which the first player is trying to maximise player $i$'s $\LEX(\parity,\mp)$ objective, and the adversary is trying to minimise this same objective (thus each state is labeled by a single weight and a single priority). We are required to show that such games are determined. To this end, consider the zero-sum game played on weighted arena $A$ in which player $i$ is trying to achieve his parity objective \emph{and} maximise his mean-payoff (and the opponent is adversarial). These are called mean-payoff parity games, and are known to be determined~\cite{DBLP:conf/lics/ChatterjeeHJ05}. Let $v_i \in \mathbb{R} \cup \{-\infty\}$ be the value of this game. Let $w_i$ be the value of the zero-sum game played on weighted arena $A$ in which player $i$ is trying to maximise his mean-payoff (i.e., he ignores his parity condition). The value of $A_i$ is equal to $v_i$ if $v_i \neq -\infty$, and equal to $w_i$ if $v_i = -\infty$.

Third, the promised reduction works as follows: player $i$ plays optimally in $A_i$ (thus ensuring at least $v_i$), but the moment a player, say $j$, deviates from this strategy all the other players punish $j$ by ensuring that player $j$ receives no more than $v_i$. To define this profile formally, let 
$\sigma_i$ be an optimal strategy for player $i$ in $A_i$ (note that it is also a strategy in $A$), 
let $\sigma^{-i}$ be an optimal strategy for the opponent in $A_i$, and let 
$val_i(v)$ be the value of the game $A_i$ starting in vertex $v$. These exist by determinacy. 
Let $\rho_j^{-i}$ be the strategy for player $j$ in $A$ derived from $\sigma^{-i}$. Let $\pi$ be the path induced by the strategy profile $(\sigma_1, \cdots, \sigma_N)$. Since $\sigma_j$ is optimal, we get that $v_j \leq pay_j(\pi)$.  

Define a strategy profile $\bar{\xi}$ where $\xi_i$ follows $\sigma_i$ as long as the play stays on $\pi$; the moment the play deviates, say in state $v$ on player $j$'s move, the strategy $\xi_i$ switches to $\rho_j^{-i}$ forever. To see that $\bar{\xi}$ is a NE reason as follows: let $\tau_j$ be any strategy for agent $j$ different from $\xi_j$ and consider the play $\pi'$ induced by $(\xi_1, \cdots, \xi_{j-1}, \tau_j, \xi_{j+1}, \cdots, \xi_N)$ starting from initial state $\iota$. Let $v$ be the vertex of player $j$ where $\tau_j$ first deviates from $\xi_j$. By construction of $\bar{\xi}$, we have that $pay_j(\pi'') \leq val_j(v)$ where $\pi''$ is the suffix of $\pi'$ starting with the deviation. Since the objectives are prefix-independent, we get that
$pay_j(\pi') = pay_j(\pi'')$. Since player $j$ played optimally until $v$, we get that $val_j(v) = val_j(\iota)$. Thus, $pay_j(\pi') \leq pay_j(\pi)$, as required.


The only thing left to check is that $\LEX(\parity,\mp)$ is prefix-independent. This follows from the definition of the lexicographic order and the fact that both the parity condition and the mean-payoff condition are prefix-independent, i.e., if the smallest priority occuring infinitely often in $\pi$ (resp. $\pi'$) is even (resp. odd), then for every $h$, the smallest priority occuring infinitely often in $h \cdot \pi$ (resp. $h \cdot \pi'$) is even (resp. odd); and if $\mp(\kappa(\pi)) < \mp(\kappa(\pi'))$ then, for every $h$, $\mp(\kappa(h \cdot \pi)) < \mp(\kappa(h \cdot \pi'))$. 
\end{proof}
