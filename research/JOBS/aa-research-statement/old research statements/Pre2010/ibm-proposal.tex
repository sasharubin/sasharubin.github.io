\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amscd}
\usepackage{amsfonts}
\usepackage{graphicx}%
\usepackage{fancyhdr}
\input{rubin.macro}

\theoremstyle{plain} \numberwithin{equation}{section}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}{Conjecture}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{finalremark}[theorem]{Final Remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}
\newtheorem{question}{Question} \topmargin-2cm

\textwidth6in

\setlength{\topmargin}{0in} \addtolength{\topmargin}{-\headheight}
\addtolength{\topmargin}{-\headsep}

\setlength{\oddsidemargin}{0in}

\oddsidemargin  0.0in \evensidemargin 0.0in
\parindent 1em
\pagestyle{plain}

\lhead{Research Proposal} \rhead{January 2008}
\chead{{\large{\bf Sasha Rubin}}} \lfoot{} \rfoot{} \cfoot{\thepage}

\begin{document}

%\raisebox{1cm}
\thispagestyle{fancy}

\begin{center}
{\bf Is Kolmogorov complexity a reasonable parameter for parameterised complexity?}
\end{center}

\noindent
I propose to study whether the Kolmogorov complexity of a finite object (e.g., a string, or a graph) explains the computational costs of deciding certain properties of that object (e.g, $k$-colourability).


%{\bf Plan B}: I propose to study whether the Kolmogorov complexity of a finite object (e.g., a string, or a graph) is a good predictor of the efficiency of deciding whether the object has certain properties (e.g, bipartite, $k$-colourable).

\smallskip

An object with high Kolmogorov complexity (KC) does not possess certain properties; namely, those properties that could otherwise be used to compress it. On the other hand, an object has low KC ostensibly because it does possess certain properties that can be used to compress it. How well can this information be exploited in the algorithmic task of deciding whether or not the object has a certain property?

\smallskip


Identifying objects with graphs, say, I propose to study which graph properties are possessed by all graphs with high (or low) KC.
%This information can be used to save some time in finding the largest clique of the graph.
%Which graph properties can (or alternatively can not) be used to give shorter descriptions of a graph compared with simply listing all its edges?
Is there a characterisation in terms of a logical formalism?
Which naturally occurring properties are covered?
Is there a relationship to properties that hold in almost all
graphs? Does computational cost display a phase transition when inputs are parameterised by (computable approximations of) KC?
%Intuitively, if a property fails to hold in almost all graphs, then it holds in almost all graphs with high KC.


\iffalse
\begin{question}
Fix the collection $\C$ of strings whose KC is within some range. Which properties can be efficiently decided on strings from $\C$, and which properties can not?
\end{question}

%For instance consider strings $w$ such that $KC(w|l(w)) \leq c$ for fixed $c$.

General question 2: What is the relationship between structural properties of an object and its KC? ]
\fi

\bigskip

%[give dfns of kc and fpt]

\noindent
{\bf Background}: Kolmogorov complexity is a measure of information content of a finite string \cite{LiVi97}. Roughly, this is the size $K(w)$, measured in bits, of the shortest program that produces the string $w$. This notion can be extended to other objects, such as infinite strings and graphs. Intuitively, an object with high KC (say $K(w) \geq |x|$) does not possess properties that could be exploited to provide it a short description (that is, compress it). This idea has found applications in proving lower bounds in complexity theory, average case analysis of algorithms, formal language theory and random graphs. Although KC is uncomputable, there have been recent successes using standard compression algorithms to approximate it \cite{LCLMV03}.

\smallskip

Parameterised complexity is a framework that classifies problems by the amount of resources (time, space) required by algorithms solving them \cite{DoFe99}.
%It measures complexity in terms of input size {\it and} a parameter.
The main idea is that problems often come with parameters that can be exploited. In some cases the parameter is implicit in the input (for instance the size of a formula), and in some cases it naturally occurs in the problem (e.g., is there a clique of size at most $k$?).
%This is usually motivated by cases where the parameter is comparatively smaller than the input size (for instance in the realm of databases, a query produced by a human is typically very small compared to the size of the database).
%The central class is fixed-parameter tractability which refers to those problems whose complexity can be split into a polynomial - that depends on the size of the input -  and some arbitrary function that depends on the parameter.

\smallskip

%One way to deal with intractability (e.g., NP-completeness) is to exploit the fact that real life data (e.g., a database query specified by a human) often has some underlying structure.
%One motivating question for this proposal is whether this underlying structure can measured in terms of KC.

The aim of this proposal is to analyse typical algorithmic problem from computer science with well studied complexity (e.g., known to be NP-complete) where the parameter is taken to be the KC of the input. For which values of KC is the problem provably easier to solve? And can computable approximations be exploited in identifying easy/hard instances of the problem?

\bigskip

\noindent
{\bf Starting point:}
Theoretical computer science is full of results of the sort: Certain intractable algorithmic problems become feasible when restricted to certain subclasses.
 %For instance,
%if parameterised by some structural information of the input graph. For instance $k$-dominating set is fixed-parameter tractable for the class of planar graphs, while it is unlikely
%that this is the case for the class of all graphs.
%find better example. simpler (no k) and intractable in general case.
For instance, Courcelle's theorem states that for every property expressible in monadic second-order logic, there is a linear time algorithm deciding if a graph of tree-width $k$ has the property \cite{cour89}.
%Similarly, first-order properties are fixed-parameter tractable for graphs of bounded local tree-width \cite{frgr02}.

The lesson is that regularity of a graph
(e.g., bounded tree-width, bounded clique-width, low diameter, \ldots) can be exploited to get tractability. But, loosely speaking, KC is a measure of {\it all} (computable) regularity in a graph. The hope is that KC, or a resource-bounded variant, can formally capture this relationship between regularity and tractability.

\smallskip

This leads to the following intuition: certain problems can be solved more efficiently on graphs with lower KC. There are a host of possible empirical experiments that can test this intuition.

%start by identifying naturally occuring properties that help reduce KC. Which properties can be decided efficiently on graphs with low KC? For instance, can regular or context-free properties be decided more efficiently than usual when restricted to the class of strings of low KC?
%Is knowing that a graph has low KC enough? or is the actual value also required?
    %\item A fixed property expressible in LFP logic is decidable in Ptime in the size of the graph. Is there a natural extension that is decidable in Ptime on all graphs of low KC?

On the other hand, a graph with high KC does not possess those properties that would allow a shorter description. Thus, simply knowing a graph has high KC could be exploited to save time/space when deciding certain properties.  For instance, a graph with high KC has no large cliques (precisely formulated in \cite{BLTV00}), useful information when searching for the largest clique.

%This leads to the second intuition: algorithmic problems on graphs with high KC can be solved faster than the worst case.

%\begin{enumerate}
%\item Which naturally occurring graph properties do not occur in graphs with high KC? Can they be characterised logically?
%\end{enumerate}

%Unify different possible parameterisations of a problem?


%In summary, this proposal considers whether KC is a good explanation for typical results in complexity theory that state that certain properties can be feasibly decided on certain structures.

\medskip

\noindent
{\bf Related work}:
Computationally hard problems sometimes display phase transitions around a critical value of a suitable parameter of the input. For instance, when 3-SAT is parameterised by the ratio of clauses to variables, the problem seems to become harder as the parameter gets closer to $\approx 4.2$. Common explanations look at structural properties of the formula itself \cite{DBM00} (number of occurences and signs of variables), or structural properties of the solution space of a formula \cite{AcRT06,GKMP06} (the solution space of a formula is the set of satisfying assignments, viewed as bit strings, with an edge between two assignments that differ by a single bit flip). The general picture seems to be that easy instances of SAT display good structural properties, while hard instances can be arbitrary.

It is intriguing whether this analysis of sets of randomly generated inputs can be explained in terms of KC of individual inputs.


%From {\it The Connectivity of Boolean Satisfiability:
%Computational and Structural Dichotomies \cite{GKMV06}}
%\begin{quote} Our work addresses the question: when does the solution graph of a Boolean formula have nice structure? To answer this question, one must clarify what is meant by nice structure. One can define it in terms of graph theoretic properties of the solution graph. We can ask what kinds of graphs are possible as solution graphs of a Boolean formula. One can focus on specific structural properties such as diameter of each component. Alternatively, once can view the Boolean formula as an implicit description of the solution graph, and study the computational complexity of algorithmic tasks such as finding if the graph is connected. Surprisingly, we show that many of these properties, both structural and algorithmic, are remarkably aligned and result in the same dichotomies. \end{quote}

% sat/unsat of formula is related to conectivity ppts of space of all solutions of formula. is kc of formula related to this space?
%hardness is related to number of variables that occur more positively than negatively, and by how much.

%
%It would be interesting to empirically test a problem like 3-SAT for a phase transition when parameterised by (an approximation to) KC; as well as see if there is a correlation between instances of 3-SAT parameterised by ratio of clauses to variables and parameterised by KC.


\iffalse
For instance, a {\it testing} algorithm distinguishes, with high probability, between the case that the graph has the property, and the case that it is far from having the property according to some measure (usually a Hamming-distance) on graphs.
\fi


\begin{small}
\bibliographystyle{alpha}
\bibliography{kc}
\end{small}
\end{document}


\smallskip

