\section{Methodology}

\subsection{Research Streams}
The scientific work in \project will be methodologically structured into 3 broad research streams:
\begin{itemize}
\item \textbf{Principles and Foundations} that will deal with the
scientific foundations of white-box self-programming mechanisms. 

\item \textbf{Algorithms and Tools}  that will deal with the
development of practical algorithms, optimizations and tools for realizing
%white-box self-programming 
such mechanisms. 

\item \textbf{Applications and Evaluation}  that will evaluate white-box self programming mechanisms in the three business critical driving applications  mentioned above.
\end{itemize}
The \textbf{Principles and Foundations} and \textbf{Algorithms and Tools} streams
will cut across 5 workpackages (WPs) roughly corresponding to the 5
objectives above.  The \textbf{Applications and Evaluation} stream
will be further refined into 3 separate WPs for the 3 driving
applications. 



\subsection{Workpackages}


Research will be organized in five scientific WPs, and three applicative WPs. The five scientific WPs will handle all objectives by  looking first to the representation; then to self-programming in the core case in which mechanisms are finite state; then the advanced case in which mechanisms have a FOL state and hence need to be abstracted to finite state before self-programming  can be performed; then will focus specifically at mechanisms that are component-based; and finally we will look at the integration of ML and stochastic decision making techniques into the framework.




\subsubsection*{WP1: Representing mechanisms and their operation domains}
This WP addresses the objectives of \textbf{making white-box self-programming mechanisms formally verifiable and  comprehensible to
    humans}.

% \project will take from \textbf{Planning} in AI \cite{GeffnerBo13} (which in turn
% coming from \textbf{Reasoning about Action} in KR \cite{Reiter01}) the idea of having
% \textbf{human-comprehensible} descriptions of the domain (the mechanism and its
% capabilities) and of the goals (the task specifications). 
% %%
% However
% instead of considering simple domain specifications, as e.g., in PDDL, \project
% will look at extensions that use \textbf{rich semantic descriptions} from
% Knowledge Representation and \textbf{componentization} from behavior
% compositions in Reasoning about Action,
% %%
% to which the PI has contributed significantly in the
% years
% \cite{DeGiacomoRS98,DeGiacomoLL00,DeGiacomoLS01,SardinaGLL04,SardinaG09,DeGiacomoLP10,DeGiacomoLM12,DeGiacomoLPV14,DeGiacomoLPV16,BanihashemiGL17}. 
% % From
% % such work, \project will draw key ideas on how to represent
% % mechanisms, the domain in which they are operating, and the properties
% % of interest in a high-level human-comprehensible fashion. 
% %%
% Particular attention will be given to computational effectiveness, in
% line with some recent exploratory work by the PI
% \cite{DeGLP16,DeGiacomoLPS16,CDMP17}.  
% %%
% %%
% Notice that on the other hand \project does not aim at defining new
% concrete representation languages. Instead it intends to use
% well-known formalisms such as BPMN, UML, OWL, etc. as concrete
% languages, though with a precise formal semantics to allow for
% automated reasoning, verification and synthesis, see e.g.,
% \cite{BerardiCG05,DeGiacomoOET17}.
% %%
% % Clearly specifications and replanned behaviors must be formally represented so as to be
% %   \textbf{verifiable}, e.g., by model checking,
% %   possibly modulo theories \cite{EiterGS10}.
% %   This is a crucial step towards the understandability required by a
% %   \textbf{white-box approach}.  In this way mechanisms can be checked, e.g.,  to \textbf{understand} if important safety conditions are
% %   satisfied. In tackling this aspect, \project will also leverage on
% %   recent advances of model checking of autonomous
% %   agents \cite{Wooldridge09,LomuscioQR17}.


% \textbf{Make white-box self-programming mechanisms verifiable} Language

\subsubsection*{WP2: Finite-state white-box self-programming mechanisms}
This WP addresses the objective of  \textbf{equipping mechanisms with general self-programming abilities}


% Another crucial difference wrt Planning is that, instead of
% considering simple planning tasks, \project will make use of the
% mathematically elegant theory of Reactive Synthesis \cite{PnRo89}
% developed in \textbf{Verification and Synthesis} in the last 30 years
% \cite{EhlersLTV17}, which however has not found widespread practical
% application because of the \textbf{intrinsic difficulties} of certain
% algorithms and constructions \cite{TsaiFVT14,DFogartyKVW13}.
% %
% We aim at \textbf{sidestepping these difficulties all-together}, by
% focusing on non-traditional kinds of specification formalisms proposed
% recently in, e.g., Reasoning about Action and Generalized Planning,
% such as LTL and LDL on finite traces, recently proposed by the PI
% together with Moshe Vardi (Rice U, Huston) \cite{DeVa13,DeVa15,DeVa16}
% and adopted in generalized forms of Planning in AI
% \cite{TorresB15,CamachoTMBM17} and in declarative business processes
% in BPM \cite{AalstPS09,DeGMGMM14,DeGMMP17}, as well as safe/co-safe
% LTL/LDL formulas, which have been shown to be more expressive than
% expected while remaning \textbf{well-behaved}
% \cite{FinkbeinerS13,FiliotJR11,Lacerda0H15,FaymonvilleFRT17}. Solvers
% for these are substantially simpler that for general reactive
% synthesis, being based on reachability and safety games, which are
% amenable to efficient implementations.

% % will adopt
% % rich temporally extended objectives such as those adopted in
% % mathematically elegant theory of \textbf{Reactive Synthesis}
% % \cite{PnRo89} developed in Verification and Synthesis in the last 30
% % years \cite{EhlersLTV17}, which however has not found diffused
% % practical application because of the \textbf{intrinsic difficulties}
% % of certain algorithms and constructions
% % \cite{TsaiFVT14,DFogartyKVW13}.  We aim at \textbf{sidestepping these
% %   difficulties all-together}, by focusing on non-traditional kinds of
% % specification formalisms proposed recently in, e.g., Reasoning about
% % Action, that reduces to reachability and safety games which are
% % amenable to efficient implementations.
% % %%




% Moreover, like in Planning, but differently form Reactive Synthesis,
% we expect mechanisms to be able to handle quickly and efficiently most
% cases, i.e., those cases that do not require to solve difficult,
% ``puzzle-like'', situations. Indeed while the Planning community has
% concentrated on simpler forms of process synthesis (reachability of a
% state of affairs), it has developed a sort of \textbf{science of search
%   algorithms for Planning}, which has brought about exceptional
% scalability improvements (orders of magnitude) in the last decade
% \cite{PommereningHB17,SteinmetzH17,LipovetzkyG17,DeGMMP17}. \project
% intends to devise new algorithms for solving reachability and safety
% games that are based on heuristic search as adopted in Planning, but
% also considering symbolic techniques adopted in synthesis by model
% checking \cite{BloemJPPS12}.
% %%
% The PI has been pioneering cross-fertilization of Planning and
% Synthesis since the end of the '90
% \cite{DeGiacomoV99,CalvaneseGV02,SardinaGLL06,DeGiacomoFPS10,PatriziLGG11,DeGMMP17}. More
% recently the PI has established \textbf{tight connections between
%   synthesis and generalized forms Planning}
% \cite{HuG11,HuG13,DeGiacomoMRS16,BDGR17} as well as between Planning
% and Behavior Composition
% \cite{SardinaG08,DePS13,DeGGPSS16,CalvaneseGLV16}.  

% \textbf{Make white-box self-programming mechanisms verifiable} techniques
% \textbf{Make self-programming abilities available while in operation} 

\subsubsection*{WP3: FOL-state white-box self-programming mechanisms}
This WP addresses the objective of  \textbf{making self-programming mechanisms data-aware}





\subsubsection*{WP4: Component-based  white-box self-programming mechanisms}
This WP addresses the objective of  \textbf{supporting component-based approaches.} 



\subsubsection*{WP5: Integrating stochastic decision and reinforcement learning} 

This WP addresses the objective of  \textbf{allowing learning and stochastic decisions, while remaining within safe bounds}.


\subsubsection*{WP6: Application and Evaluation}



\subsection{Phases}
\begin{itemize}
\item \textbf{Core Science}
\item \textbf{Refinement and Engineering}
\item \textbf{Towards Practical Adoption}
\end{itemize}








\endinput










We discuss each of them in details below.

\subsection{ADDRESSING OBJECTIVES}


\subsubsection{Addressing objective 1: Make white-box self-programming mechanisms
  comprehensible to humans.}

The key property of white-box self-programming mechanisms is the
ability of describing their specification, the programs they generate
and the relationship between the two in human terms.

To do so the domain where the mechanism operates, the 
mechanism itself, its capabilities and limitations, as well as
its specifications and goals need to be formally described in terms of concepts that
can be shared with humans.

\project will consider this issue upfront, by founding white-box self-programming mechanisms on the area of AI called \textbf{Knowledge Representation}. 

\begin{quote}\it
The PI is  a prominent member of the scientific community working on Knowledge Representation. He is indeed the single researcher with most papers appeared in the Flagship Conference of the community: the International Conference on Principles of Knowledge Representation and Reasoning (KR) - ranked A* according to CORE.
\end{quote}


Knowledge Representation stems from a deep
tradition in logic. In particular, it aims at building systems that
know about their world they operate in and are able to act in an informed way in it,
as humans do.
%%
A crucial part of these systems is that knowledge is represented
``\emph{symbolically}'', and that ``\emph{reasoning procedures}'' are able to extract
consequences of such knowledge as new symbolic representations. Such
an ability is used to deliberate in an informed fashion the course of
actions to take, understanding the consequences of the action performed.

In \cite{Leve14,Leve17}, it is argued that Knowledge Representation is
radically new idea in human history. It comes about after a long
gestation, stemming from Aristotle, who developed the initial notion
of logic though unrelated to notion of computation; continued by
Leibniz, who brought forward a notion of ``thinking as computation'',
though not yet symbolic; and later by Frege, who developed the notion
of symbolic logic, though unrelated to computation; and finally by the
breakthrough in human thinking of the early part of last century with
Church, Godel, and Turing, who set the bases for symbolic logic bound
together with computation and ultimately for Computer Science, though
even them did not think about logic as a way of representing
knowledge. The use of a symbolic systems to represent knowledge
expressed in terms of human concepts can only be traced back to
McCarthy's 1959 seminal work on Artificial Intelligence \cite{McCa57}.

Knowledge Representation has developed enormously since then in
diverse directions and subareas. 

%%
\project will build on work on two subareas in particular:
\begin{itemize}
\item \textbf{Description Logics and Ontology-Based Data Access}
\item \textbf{Reasoning about Actions}
\end{itemize}

\textbf{Description Logics} are the formalism of election in for
representing the information on the domain of interest in terms of
objects grouped in classes or concepts and relationships among such
classes.  Moreover \textbf{description logics} are nowadays considered
the standard formalism for ontologies and deeply shaped semantic
technologies including the current W3C standard OWL 2.

The PI has deeply contributed in the development of description
logics. First he worked on the correspondence between expressive
description logics and modal logics of programs such as Dynamic Logic \cite{KR06,PODS98}.
Then more recently, together with his group in Rome he developed one
of the best know light weight description logics, DL-lite \cite{JODS07}, which is essentially
able to formalize UML class diagrams and Entity-Relationship diagrams,
while keeping inference and conjunctive query answering tractable (the
latter in AC0, the same cost as standard SQL). DL-lite in turn made it possible to develop \textbf{Ontology Based Data Access}, which can be considered the most successful use of semantic technologie for data integration \cite{PoggiLCGLR08,SequedaM17,Statoil17}. 

The role of these technologies in \project is to represent in terms of an ontology chosen so has to capture the doman on interest in which the mechanism is operating as well as the (high-level) data structures of the mechanism itself in terms that are sharable by humans as e.g., in \cite{TenorthB17}. In particular \project intends to exploiting the techniques for efficient query-aswering provided by DL-lite and Ontology Based Data Access, especially in the write-also variants, explored recently \cite{ISWC16,ISWC17}.






\textbf{Reasoning about Action} takes a first-person view of an agent. %
\footnote{This contrast with the work in Multi Agents Systems, where
  typically a third person view (a ``designer'' view) is adopted.} %
The agent as a representation of the domain in which it operates;
a  representation of 
 the possible actions in terms of preconditions and effects formally expressed over the representation of domain;
finally it has a representation of  complex agent behaviors and capabilities typically  described through high-level programs, whose atomic instruction and tests corresponds, respectively, to actions and queries over the representation of the domain.
%%
Through 
reasoning on these representations the agent undestand what doing an action or enacting a certain behavior will bring about, enabling it to make informed decisions on which action to choose or to exclude in relation to its current tasks/goals/specifications.
%%
Reasoning about actions has been studied in depth through
comprehensive frameworks, such as that of Situation Calculus \cite{McHa69,Reiter01}.

The PI has deeply contributed to the development of Reasoning about
Action. In particular, within the framework of Sitation Calculus. Specifically, the PI
introduced of the high-level programming languages
ConGolog and IndiGolog, the
distinction between off-line executions and online executions, the
notion of execution monitoring and recovery, interleaving execution
and planning, the notion of ability \cite{DeGiacomoRS98,DeGiacomoLL00,SardinaGLL04,SardinaGLL06}.

It is important ot notice that the work Reasoning about Action in
Situation Calculus as well as in most other frameworks, is based on a
\textbf{First-Order Representation of the State}. In other words the
current state of the Agent is represented in terms of
predicates/relations.  This give rise to infinite transition systems
which are typically impossible to verify directly.

Recently the PI has devised a set of novel results
that shows the effective computability of expressive variants of the
original full-fledged (predicate based) Situation Calculus [DeLP12,
DLPV14, DeLV14]. Such results are being complemented by the
possibility of combining action theories with ontological
representations in description logics [CDLL13, CDMP13,
HCMD14]. Moreover, the techniques for applying belief revision to
transition systems based on dynamic logic of assignments proposed
recently [HMDW14], open up the possibility of grounding
computationally the notion of “behavior revision”.

Moreover, in Reasoning about Action restricted forms of representations
(essentially propositional vs first-order) have been put forward to study efficient
action deliberation and planning, which in these years is producing a
vast array of particularly fruitful results [GeBo13, DePS10, DFPS10,
CaDH11, FeDL12, DePS13, DeVa13, GeTh14, DeDM14, DDGM14].

In fact  the standard language PDDL itself universally used for Planning is largely inspired by the research in Reasoning about Action.

\subsubsection{Addressing objective 2: Make white-box self-programming mechanisms verifiable.}

The translation from LTL to automata is exponential in the worst case, but exponential blow-up is rarely seen in practice and many optimizing compilers have been developed [46, 50, 60, 70, 76, 74, 71, 73, 144, 140, 146].




% [46] J.-M. Couvreur. On-the-fly verification of linear temporal logic. In Proc. World Congress on Formal Methods, pages 253–271, 1999.

% [50] N. Daniele, F. Guinchiglia, and M. Vardi. Improved automata generation for linear temporal logic. In Computer Aided Verification, Proc. 11th International Conference, volume 1633 of Lecture Notes in Computer Science, pages 249–260. Springer-Verlag, 1999.

% [60] K. Etessami and G. Holzmann. Optimizing bu ̈chi automata. In Proc. 11th Int’l Conf. on Concurrency Theory, Lecture Notes in Computer Science 1877, pages 153–167. Springer-Verlag, 2000.

% [76] S. Gurumurthy, R. Bloem, and F. Somenzi. Fair simulation minimization. In Computer Aided Ver- ification, Proc. 14th International Conference, volume 2404 of Lecture Notes in Computer Science, pages 610–623. Springer-Verlag, 2002.

% [74] D. Giannakopoulou and F. Lerda. From states to transitions: Improving translation of ltl formulae to bu ̈chi automata. In Proc. 22nd IFIP Int’l Conf. on Formal Techniques for Networked and Distributed Systems, pages 308–326, 2002.

% [71] P. Gastin and D. Oddoux. Fast LTL to bu ̈chi automata translation. In Computer Aided Verification, Proc. 13th International Conference, volume 2102 of Lecture Notes in Computer Science, pages 53– 65. Springer-Verlag, 2001.

% [146] X. Thirioux. Simple and efficient translation from ltl formulas to Bu ̈chi automata. Electr. Notes Theor. Comput. Sci., 66(2), 2002.

% [144] F. Somenzi and R. Bloem. Efficient Bu ̈chi automata from LTL formulae. In Computer Aided Veri- fication, Proc. 12th International Conference, volume 1855 of Lecture Notes in Computer Science, pages 248–263. Springer-Verlag, 2000.

% [140] R. Sebastiani and S. Tonetta. “more deterministic” vs. “smaller” bu ̈chi automata for efficient ltl model checking. In 12th Advanced Research Working Conference on Correct Hardware Design and Verification Methods, volume 2860 of Lecture Notes in Computer Science, pages 126–140. Springer- Verlag, 2003.


% [73] R.Gerth, D.Peled, M.Vardi, and P.Wolper. Simple on-the-fly automatic verification of linear temporal logic. In P. Dembiski and M. Sredniawa, editors, Protocol Specification, Testing, and Verification, pages 3–18. Chapman \& Hall, August 1995.


We need to go, however, beyond LTL. It is fairly clear that LTL is is not expressive enough for complex behavior over complex planning domains. Since the proposal in [125] to apply LTL to the specification and verification of reactive systems, the role of LTL as a feasible approach to that task has been widely studied. Over the past two decades an extensive research has been carried out concerning various aspects of using LTL to specify and verify reactive systems, cf. [55, 126, 153]. One of the conclusions of this research is that LTL, which consists of the temporal connectives next and until is not expressive enough for its task [113]. 

More recently, there has been a major emphasis recently in the semiconductor industry on designing industrial-strength temporal property specification languages. Two major languages are ForSpec [5] and Sugar [10], developed by Intel and IBM, respectively, which are both extensions of Pnueli’s LTL. A recent industry standard (PSL 1.1) resulted from merging features of ForSpec and Sugar (see www.accellera.org). All these languages extend LTL significantly. Many of additional constructs of these languages reflect current semiconductor design methodology. The most basic extension, however, is the addition of regular-event construct. For example, in ForSpec one can write e1 triggers e2, where e1 and e2 are regular expressions, which asserts that a behavior matching a word in e1, should be followed by a behavior matching a word in e2. One one hand, it is shown in [5] that such extension remedies well-known deficiencies of LTL [113, 159]. On the other hand, this extension is extremely easy to use; validation engineers prefer using regular expressions to using nested expressions of next’s and until’s. The extension of LTL with regular expression is known as RELTL. While the automata-theoretic approach has been extended to RELTL [4], there is no published work on optimized compilation of RELTL to automata. We plan to ex- plore the applicability of RELTL to specifying complex behaviors in planning settings. If RELTL is found to be appropriate for such settings, then we need to extend optimized compilation techniques from LTL to RELTL


% [125] A. Pnueli. The temporal logic of programs. In Proc. 18th IEEE Symp. on Foundation of Computer Science, pages 46–57, 1977.

% [55] E. Emerson. Temporal and modal logic. In J. V. Leeuwen, editor, Handbook of Theoretical Computer Science, volume B, chapter 16, pages 997–1072. Elsevier, MIT Press, 1990.

% [126] A. Pnueli. Applications of temporal logic to the specification and verification of reactive systems: A survey of current trends. In Proc. Advanced School on Current Trends in Concurrency, pages 510–584, Berlin, 1985. Volume 224, LNCS, Springer-Verlag.

% [153] M. Vardi. Branching vs. linear time: Final showdown. In Proc. Tools and Algorithms for the Con- struction and Analysis of Systems (TACAS), volume 2031 of Lecture Notes in Computer Science, pages 1–22. Springer-Verlag, 2001.

% [113] O. Lichtenstein, A. Pnueli, and L. Zuck. The glory of the past. In Logics of Programs, volume 193 of Lecture Notes in Computer Science, pages 196–218, Brooklyn, June 1985. Springer-Verlag.

% [5] R. Armoni, L. Fix, A. Flaisher, R. Gerth, B. Ginsburg, T. Kanza, A. Landver, S. Mador-Haim, E. Singerman, A. Tiemeyer, M. Vardi, and Y. Zbar. The ForSpec temporal logic: A new tempo- ral property-specification logic. In Proc. 8th International Conference on Tools and Algorithms for the Construction and Analysis of Systems, Lecture Notes in Computer Science 2280, pages 296–211. Springer-Verlag, 2002.

% [10] I. Beer, S. Ben-David, C. Eisner, D. Fisman, A. Gringauze, and Y. Rodeh. The temporal logic sugar. In Proc. Conf. on Computer-Aided Verification, LNCS 2102, pages 363–367, 2001.

% [159] P. Wolper. Temporal logic can be more expressive. Information and Control, 56(1–2):72–99, 1983.

% [4] R. Armoni, D. Bustan, . Kupferman, and M. Vardi. Resets vs. aborts in linear temporal logic. In 9th Int’l Conf. on Tools and Algorithms for the Construction and Analysis of Systems, pages 65–80, 2003.




\subsubsection{Addressing objective 3: Equip mechanisms with general self-programming abilities.}


The recent advances in synthesis algorithms and methodology have resulted in promising case studies generating a lot of excitement in programming languages and embedded software research communities. The goal of \project is to build on the momentum of these recent developments to bring together ideas in these various point solutions into a transformative paradigm of self-programming mechanisms engineering. In this vision of synthesis-centric software design, instead of viewing synthesis as a problem of automatically mapping a high-level specification to a low-level implementation, we focus on the problem of consistently integrating distinct artifacts describing different views of the design via a collaborative effort between the mechanism execution and the synthesizer.



In our view of computer-augmented program engineering, the problem of synthesis gets formalized in a variety of ways: deriving an executable implementation from a declarative specification; completing a partial implementation guided either by logical assertions or by example behaviors; refining an implementation to satisfy additional functional and/or optimality requirements; and deriving and/or combining interface specifications from legacy components and libraries. A key ingredient in all these problems is the adversarial interaction between the uncontrolled aspects of the system (such as the environment, already specified parts, libraries to be used) and the controlled aspects to be designed (such as program components and parameters) which can be formally modeled as quantifier alternation: the synthesis question is whether there exists a control strategy such that for all system behaviors, the completed (or “closed-loop”) system satisfies the specification. This game-theoretic perspective distinguishes synthesis from post-facto verification, where we check if every behavior of the implementation satisfies the specification.


The algorithmic foundations necessary to solve the synthesis problem are rooted in game theory, logic, automata theory, program analysis, and control theory. While formal verification has been considered by many an impossible dream, it has recently become an industrial reality [6], driven by the synergy between theory and practice, where several practical algorithmic advances arose out of theoretical research results. For example, the automata-theoretic framework for model checking [67] is used in industrial hardware model checking [24], the timed-automaton framework and associated analysis [3] led to efficient tools for verifying real-time systems [9], and the abstraction-refinement framework [18], driven by decision procedures for expressive theories [7], is used in industrial software model checking [6]. We strongly believe that progress in computer-augmented program engineering will similarly be driven by, and drive, algorithmic foundational problems.

Recent advances in computer-augmented program engineering have used a variety of computational engines including decision procedures for constraint-satisfaction problems, computing winning strategies in games for temporal specifications, computational-learning algorithms, scalable static-analysis techniques, abstraction techniques for reducing state-spaces, and numerical simulation and optimization. Advances in these core computational engines can be shared by different synthesis algorithms in the context of different applications. With this motivation, we outline a few representative directions for future research.

Specifications for reactive systems such as protocols, are usually given in a temporal logic: a logic that express properties of ongoing behaviors. The synthesis problem for reactive systems goes back to work in the 1960’s [15], with finite automata on infinite words and trees providing the crucial algorithmic apparatus, with some recent efforts to translate these results into practice [12, 42]. There are a number of challenging open theoretical questions relating to reactive synthesis, such as the complexity of solving parity games on graphs and the complexity of solving stochastic games [23]. We also propose to develop theoretical foundations for synthesis in presence of quantitative objectives to allow selection, by users, among alternative implementations using the degree to which they satisfy the specification and perform trade-off analysis between how well the specification is satisfied and the complexity of the implementation. Finally, there is a clear need for developing practical heuristics to address the high computational complexity of reactive synthesis problems, just as the state-explosion problem in model checking was mitigated by a host of heuristics [16]. Potential directions include the combination of symbolic techniques with learning algorithms, and incremental synthesis algorithms to process evolving requirements.


\textbf{Robustness and automated exception handling.} In many application domains, such as the robotic systems and automotive control software, robustness is as important as correctness. By robustness we mean the capability of a program to perform “adequately”, even when the assumptions made at design time do not hold, for example, due to limited knowledge about the environment. The key challenge is how to define what constitutes adequate behavior. By drawing inspiration from the well established area of robust control, the PIs [49] have recently shown that it is possible to synthesize software that is robust against unspecified disturbances. This is a significant advance since existing techniques ensuring fault tolerance, or other types of robustness against disturbances, require the explicit modeling of all the faults that can occur in the system. Unfortunately, such explicit adversarial modeling of disturbances leads to a game that is computationally expensive to solve. In contrast, the results in [49] lead to games of polynomial complexity to construct programs that are robust against unmodeled disturbances of bounded power. The key insight is to use the programmer knowledge to derive a metric on program states that can be used to quantify the effect of disturbances of bounded power. In this expedition we will start from these recent advances and build the foundations of robust programming by synergistically drawing inspiration in control theory, and robust control in particular.

\subsubsection{Addressing objective 4: Make self-programming abilities available while in operation.} 

The automata-theoretic constructions studied in the literature all operate at the propositional level; that is, the temporal logics studied are propositional. An issue that has not been studied is the cost of “Booleanization”. Taking, for example, block-world planning problems [87], the number of Boolean variables tend to grow superlinearly (n3) with the number of blocks. Thus, just generating goal formulas becomes computationally challenging for a block world with 20 blocks. (It may seen that a cubic blow-up is rather benign, but one needs to remember that this blow-up occurs before we even start temporal reasoning, and therefore may have heavy computational cost.) In contrast, TLPLAN [8], which uses “formula decomposition”, avoids Booleanization, and can handle 20-blocks problems easily. The key insight in TLPLAN is that one can express control information in first-order LTL, search for a plan by simple depth-first forward chaining, and use formula-decomposition techniques to dynamically check formula on that component of the search space that we have generated “on-the-fly”.

In contrast, TLPLAN [8], which uses “formula decomposition”, avoids Booleanization, and can handle 20-blocks problems easily. The key insight in TLPLAN is that one can express control information in first-order LTL, search for a plan by simple depth-first forward chaining, and use formula-decomposition techniques to dynamically check formula on that component of the search space that we have generated “on-the-fly”.
This seems to imply that a major benefit is using LTL to specify control knowledge. 

Thus, to adapt automata-theoretic technique to planning we need on-the-fly techniques that combine on- the-fly compilation of temporal connectives, as in [45, 50, 73], which are significantly more sophisticated that the somewhat ad-hoc formula-decomposition technique used in TLPLAN, with on-the-fly variable- instantiation techniques used in TLPLAN. The key is to avoid prior Booleanization. We also need to compare the techniques used in TLPLAN with techniques used in temporal databases for processing non- propositional temporal integrity constraints [33, 34].

% [8] F. Bacchus and F. Kabanza. Using temporal logics to express search control knowledge for planning. AI J., 116(1–2):123–191, 2000.

% [87] H. Kautz and B. Selman. Unifying sat-based and graph-based planning. In Proc. 16th Int’l Joint Conf. on Artificial Intelligence, pages 318–325. Morgan Kaufmann, 1999.

% [45] C. Courcoubetis, M. Vardi, P. Wolper, and M. Yannakakis. Memory efficient algorithms for the verification of temporal properties. Formal Methods in System Design, 1:275–288, 1992.

% [50] N. Daniele, F. Guinchiglia, and M. Vardi. Improved automata generation for linear temporal logic. In Computer Aided Verification, Proc. 11th International Conference, volume 1633 of Lecture Notes in Computer Science, pages 249–260. Springer-Verlag, 1999.

% [73] R.Gerth,D.Peled,M.Vardi,andP.Wolper.Simpleon-the-flyautomaticverificationoflineartempo- ral logic. In P. Dembiski and M. Sredniawa, editors, Protocol Specification, Testing, and Verification, pages 3–18. Chapman & Hall, August 1995.

% [33] J. Chomicki. Efficient checking of temporal integrity constraints using bounded history encoding. ACM Trans. Database Syst., 20(2):149–186, 1995.

% [34] J.Chomicki and D.Niwinski. On the feasibility of checking temporal integrity constraints. J. Comput. Syst. Sci., 51(3):523–535, 1995.

Many real-world planning domains are inherently nondeterministic, that is, the result of actions are not fully predictable. For example, a robot trying to pick up a block may fail to grip the block. Formally, we need to consider a set-valued transition function $R : W \times Act \rightarrow 2^W$ , which assigns to each state and action a set of possible successor states. One can still can search for sequential condition plans, but in general the solution to the planning problem in such settings require universal plans [121, 137]. We consider here the most general universal plans, where the action selected depends not only on the current observations but on the sequence of observations made so far. This means that the plan should be viewed as a general reactive program [79], engaged in an ongoing interaction with the system it is trying to control. A correct plan should be able to handle arbitrary behavior of the system.

% [121] M. Peot and D. Smith. Conditional nonlinear planning. In Proc. of 1st Int. Conf. on AI Planning Systems (AIPS’92), pages 189–197. Morgan Kaufmann Publisher, 1992.

% [137] M. Schoppers. Universal plan for reactive robots in unpredictable environments. In Proc. Int. Joint Conf. on Artificial Intelligence, 1987.

% [79] D. Harel and A. Pnueli. On the development of reactive systems. In K. Apt, editor, Logics and Models of Concurrent Systems, volume F-13 of NATO Advanced Summer Institutes, pages 477–498. Springer-Verlag, 1985.


In its full generality, a plan is a mapping $p : Obs^∗ \rightarrow Act$, which selects an action based on the full history of observations. The plan $p$ realizes a specification $\varphi$  if all traces of $p$ satisfy $\varphi$. 
It turns out that the problem of synthesizing universal plans in nondeterministic domains has a long history in mathematical logic, going back to the 1960s [35, 18, 130]. More recently, it has been studied by computer scientists [6, 1, 52, 80, 104, 127, 128, 149, 151], as well as by control theorists [3, 132, 147]. The automata-theoretic approach is the key to modern algorithmic approaches to this problem. 

% [35] A. Church. Logic, arithmetics, and automata. In Proc. International Congress of Mathematicians, 1962, pages 23–35. institut Mittag-Leffler, 1963.

% [18] J. Bu ̈chi and L. Landweber. Solving sequential conditions by finite-state strategies. Trans. AMS, 138:295–311, 1969.

% [130] M. Rabin. Automata on infinite objects and Church’s problem. Amer. Mathematical Society, 1972.

% [6] A. Arora, P. Attie, and E. Emerson. Synthesis of fault-tolerant concurrent programs. In Proc. 17th ACM Symposium on Principles of Distributed Computing, pages 173–182, 1998.

% [1] M. Abadi, L. Lamport, and P. Wolper. Realizable and unrealizable concurrent program specifications. In Proc. 16th International Colloquium on Automata, Languages and Programming, volume 372, pages 1–17. Lecture Notes in Computer Science, Springer-Verlag, July 1989.

% [52] D. Dill. Trace theory for automatic hierarchical verification of speed independent circuits. MIT Press, 1989.

% [80] T. Henzinger, S. Krishnan, O. Kupferman, and F. Mang. Synthesis of uninitialized systems. In Proc. 29th Colloq. on Automata, Programming, and Languages, volume 2380 of Lecture Notes in Computer Science, pages 644–656. Springer-Verlag, 2002.

% [104] O. Kupferman and M. Vardi. Synthesis with incomplete informatio. In Advances in Temporal Logic, pages 109–127. Kluwer Academic Publishers, January 2000.

% [127] A. Pnueli and R. Rosner. On the synthesis of a reactive module. In Proc. 16th ACM Symp. on Principles of Programming Languages, pages 179–190, Austin, January 1989.

% [128] A. Pnueli and R. Rosner. On the synthesis of an asynchronous reactive module. In Proc. 16th International Colloquium on Automata, Languages and Programming, volume 372, pages 652–671. Lecture Notes in Computer Science, Springer-Verlag, July 1989.

% [149] W. Thomas. On the synthesis of strategies in infinite games. In E. Mayr and C. Puech, editors, Proc. 12th Symp. on Theoretical Aspects of Computer Science, volume 900 of Lecture Notes in Computer Science, pages 1–13. Springer-Verlag, 1995.

% [151] M. Vardi. An automata-theoretic approach to fair realizability and synthesis. In P. Wolper, editor, Computer Aided Verification, Proc. 7th International Conference, volume 939 of Lecture Notes in Computer Science, pages 267–292. Springer-Verlag, Berlin, 1995.

% [3] M. Antoniotti. Synthesis and verification of discrete controllers for robotics and manufacturing de- vices with temporal logic and the Control-D system. PhD thesis, New York University, New York, 1995.

% [132] P. Ramadge and W. Wonham. The control of discrete event systems. IEEE Transactions on Control Theory, 77:81–98, 1989.

% [147] J. Thistle. Control of infinite behavior of discrete-event systems. PhD thesis, University of Toronto, 1991.

While the standard automata-theoretic approach to universal planning yielded theoretically optimal upper bound, cf. [99, 104, 90, 127], it has proved to be not too amenable to implementation. First, Safra’s construction proved quite resistant to efficient implementation [145]. Second, the best-known algorithms for alternating tree-automata emptiness are exponential [84]. Thus, while highly optimized software packages for automata on finite words and finite trees have been developed over the last few years [53], no such software has been developed for automata on infinite trees.

Thus, to date, research in the model-checking community on program synthesis has not had much of an external impact, especially in comparison to model checking. We believe that conditions are now right for significant progress on the topic of synthesis. Basic factors underlying successful synthesis methods include the complexity of the decision procedures, and overcoming the combinatorial state-explosion problem. 

% [99] O. Kupferman and M. Vardi. Church’s problem revisited. The Bulletin of Symbolic Logic, 5(2):245 – 263, June 1999.

% [104] O. Kupferman and M. Vardi. Synthesis with incomplete informatio. In Advances in Temporal Logic, pages 109–127. Kluwer Academic Publishers, January 2000.

% [90] P. Kolaitis and M. Vardi. Conjunctive-query containment and constraint satisfaction. Journal of Computer and System Sciences, pages 302–332, 2000. Earlier version in: Proc. 17th ACM Symp. on Principles of Database Systems (PODS ’98).

% [127] A. Pnueli and R. Rosner. On the synthesis of a reactive module. In Proc. 16th ACM Symp. on Principles of Programming Languages, pages 179–190, Austin, January 1989.

% [145] S. Tasiran, R. Hojati, and R. Brayton. Language containment using non-deterministic omega- automata. In Proc. of 8th CHARME: Advanced Research Working Conference on Correct Hardware Design and Verification Methods, volume 987 of Lecture Notes in Computer Science, pages 261–277, Frankfurt, October 1995. Springer-Verlag.

% [84] M. Jurdzinski. Small progress measures for solving parity games. In 17th Annual Symposium on Theoretical Aspects of Computer Science, volume 1770 of Lecture Notes in Computer Science, pages 290–301. Springer-Verlag, 2000.

% [53] J. Elgaard, N. Klarlund, and A.Moeller. Mona1.x: new techniques for WS1S and WS2S. In Computer Aided Verification, Proc. 10th International Conference, volume 1427 of Lecture Notes in Computer Science, pages 516–520. Springer-Verlag, Berlin, 1998.



\subsubsection{Addressing objective 5: Make self-programming mechanisms data-aware.}

\textbf{Automated abstraction and refinement.} The appeal of abstraction lies in building sound abstract models from complex systems (often using automated logic engines such as SMT solvers), and subjecting the resulting models to efficient search and analysis [20]. One primary foundational goal is to find theories of abstraction for synthesis. Can we synthesize systems by synthesizing in the abstract and combine it with synthesis of refinements to build a concrete system? Can the synthesized systems be refined automatically depending on their quality? How can users improve the abstraction process? Answering the preceding questions will immediately make the powerful search-based techniques and SMT solvers come to bear on the synthesis problem.


\subsubsection{Addressing objective 6: Allow learning and stochastic decisions, while remaining within safe bounds.}


\textbf{Learning from examples.} While classical formal verification and synthesis relies on deductive reasoning and decision procedures for constraint-satisfaction problems, our view of synthesis also involves inference of program components from example behaviors, both positive and negative, using computational learning. While there is a wealth of literature on learning theory (see, for instance, [37]), we plan to explore how it can be adopted and advanced in the context of program synthesis. As an illustrative example, consider the recent interactive add-in for Microsoft Excel spreadsheet software based on the concept of “programming by examples” [30]. In this system, the user demonstrates the desired transformation task (for example, rewriting all names to a uniform format such as first-name followed by a single space, followed by last-name) using a few examples, and the synthesis tool automatically generates the “best” program (in the form of an Excel Macro) that is consistent with all the examples. The synthesized program is then used to transform all the entries in the spreadsheet, and if this synthesized transformation does not match the user’s intent, the user can guide the synthesizer by highlighting the incorrect updates, thus, providing negative examples for the subsequent iteration. While this tool is exemplary of the envisioned ExCAPE methodology, and has the potential of enormous impact by facilitating intuitive programming by millions of end users, the theoretical foundations of learning the desired program from examples are not yet understood. A suitable theoretical abstraction for the desired program is a “string-to-string” transducer. While there is a well developed theory of minimization and learning for the class of sequential transducers [51], sequential transducers are not expressive enough to capture the typical transformations that involve swapping of substrings. The recently proposed model of streaming string transducers [1] has appealing theoretical properties, such as well characterized expressiveness. Since this model can capture the desired Excel transformations, algorithms for minimization and learning—a topic for proposed research, can lead to efficient synthesis procedure with guarantees of convergence.


\subsubsection{Addressing objective 7: Favor component-based approaches.}  



\textbf{Interface-based synthesis and composition.} Interfaces capture essential properties of components while hiding irrelevant details. Interface theories [26] provide composition operators, compatibility notions (is the composition of a set of components legal?), and conditions for substitutability (when can one component be replaced by another without compromising the properties of the overall system?) that enable component-based design and alleviate state explosion. Compositionality and incrementality raise a number of questions in the context of synthesis: How to separately synthesize controllers for individual components, or for the same component but with respect to different properties, and then combine them into a single controller? How to derive the component interfaces? Because components take different forms depending on the application domain (they can be pieces of hardware, of software, or of models written in a high-level language such as Simulink or UML), there is no unique, “one-size-fits-all” interface model. For instance, interfaces carry different information in synchronous vs. dataflow components, or when reasoning about I/O dependencies vs.!correctness vs. timing and performance properties [46, 45, 66, ?]. In this expedition, domain knowledge of the human in the loop will be key in identifying the right level of abstraction and information content of the interfaces. In conjunction with this, we will develop methods to derive composite interfaces from basic interfaces automatically, thus maximizing the synergy of human and synthesizer.

\subsection{APPLICATIVE CONTEXTS}

\subsubsection{Applicative context 1: Manufacturing}

\subsubsection{Applicative context 2: Internet of Things}

\subsubsection{Applicative context 3: Business Processes}

\subsection{ORGANIZATION OF THE WORK}

\subsubsection{Streams}
\paragraph{Stream 1: Foundations}
\paragraph{Stream 2: Algorithms and Tools}
\paragraph{Stream 3: Applications and Evaluation}

\subsubsection{Phases}
\paragraph{Core Science}
% \paragraph{Core Science 1: Finite state White-Box Self-Programming Mechanisms}
% \paragraph{Core Science 2: FOL state White-Box Self-Programming Mechanisms}
\paragraph{Refinement and Engineering}
\paragraph{Towards Practical Adoption}


% \subsection{VERTICAL THEMES}
% \subsubsection{Vertical theme 0: Representing mechanisms and their operation domains}
% \subsubsection{Vertical theme 1: Finite-state mechanisms}
% \subsubsection{Vertical theme 2: FOL-state mechanisms}
% \subsubsection{Vertical theme 3: Composition}
% \subsubsection{Vertical theme 4: Integrating stochastic decision and reinforcement learning} 













\textbf{Tools.} A major component of our efforts will be the development of powerful and usable prototype tools for software synthesis. Building such prototypes is not only critical for evaluating our work, but can also provide a strong catalyst for industrial adoption of synthesis, as has occurred in the program analysis and model checking communities. Our team has significant collective expertise in translating theoretical advances to software tools. Examples include model checkers such as CheckFence [13], modeling and design environments such as and Metropolis [58], supervisory control tools for discrete-event and hybrid systems (DESUMA [56] and Pessoa [64]), robot mission and motion planning tools (LTLMOP), synthesis frameworks such the Sketch tool [60] and the VS3 tool, and domain-specific extensions of programming languages such as the Jif project for extneding the type system of Java for confidentiality and integrity security annotations [71]. We expect to create tools that span all levels of synthesis—new computational engines, tools that implement a range of synthesis methodologies, and tools that solve our proposed challenge problems, among others. Our aim is to produce not just isolated tools, but rather an ecosystem of tools that share infrastructure to the greatest extent possible. For example, we will develop common interfaces for implementations of synthesis methodologies, so that different implementations can be directly compared, and so that the same implementation can be used for multiple challenge problems. We believe the scope of this Expedition and the level of coordination among our team will enable us to create infrastructure that will be the foundation of the next generation of synthesis tools.


\textbf{Technical evaluation.} Each individual tool or new algorithm we develop will have different, targeted evaluation criteria. For example, an SMT solver can be measured in terms of how large or complex a formula it can solve; or an optimization in an encoding of a synthesis problem as an SMT formula could be measured in terms of how much the size of the SMT formula is reduced, and how much faster the new formula can be solved. For complete synthesis solutions that solve challenge problems, there are several possible evaluation criteria, both quantitative and qualitative. First, at a high level, we can evaluate how much of the problem the synthesizer solves—what class of programs can be synthesized, and what remains out of reach. Second, we can evaluate the effort required by the programmer, in terms of the size and complexity of the synthesis specification, and how long it took to derive that specification. Third, we can evaluate the resources required by the synthesis tool, in terms of time and memory; in some cases, we may also be able to measure how the tool’s performance varies depending on how rich a specification the programmer provides (e.g., a “tight” specification, with fewer possible solutions, could yield faster synthesis).








%%% Local Variables:
%%% mode: latex
%%% TeX-master: "PartB2"
%%% TeX-PDF-mode: t
%%% End: