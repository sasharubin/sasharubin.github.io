This part of the proposal should be read in conjunction with Part B1,
which is to be considered an integral part of the project description. 


\section{State-of-the-art and objectives}

% \vspace{-1ex}

% %\subsection*{MOTIVATION AND LONG TERM VISION}

% \subsection{STATE OF THE ART} % AND OBJECTIVES}

% %
% \vspace{-1ex}

We are witnessing an increasing availability of \textbf{mechanisms}
that offer some form of programmability.  Obvious examples are 
software in our computers and mobile devices, as well as intelligent
machines, such as cognitive robots, self-driving cars, flying drones,
etc., which are becoming a reality.
%%
In particular, programmable mechanisms are increasingly becoming
important in three contexts considered of pivotal importance in
%YL
%Business
today's economy,
namely \textbf{Manufacturing}, \textbf{Internet of Things},
and \textbf{Business Process Management}.  These three areas will act
as specific testbeds of the science and technology developed within
\project.


There are compelling reasons to introduce
\textbf{self-programming abilities} in these systems, such as
\textbf{self-adaptability} to changing users and environment
conditions exploiting information gathered at runtime
\cite{Seiger2016},  
%yl
%or 
and
\textbf{automated exception handling} to
suitably \textbf{recover from unexpected situations}
\cite{MarrellaMS17}. 



The current technology, e.g., \textbf{autonomic computing}, which has
long advocated self-configuration, self-healing, self-optimization,
and self-protection, however, is essentially based on
\textbf{preprogrammed solutions} by IT professionals
\cite{ibm2005autonomic}. That is, although sophisticated languages and
methodologies for streamlining 
%YL
%for writing 
the development of adaptation and exception handling recovery
procedures are available, %\cite{???},
IT professionals, software engineers and programmers, are still
writing all code by hand in the end.
%%
As a result, in spite of the progresses in the organization of the
software development process, this traditional way of tackling
automated reactions in mechanisms is showing serious limitations.
%YL
%, for several applications, for which it is 
In many application areas, it is simply
too \textbf{costly} and
\textbf{error-prone} to delegate to software engineers to list and
handle all possibly adaptation tasks that may arise in the mechanism
execution.
%%
In fact, especially when applications have to handle widely unexpected
circumstances, stemming from the interaction with the real world or with humans taking decisions based on unmodeled circumstances, as indeed in
Smart Manufacturing,  Internet of Things and Business Processes Management, it is
considered simply \textbf{infeasible} to determine apriori all
possible adaptations that may be needed at runtime
\cite{MarrellaMS17}.

In this state of affairs, the fantastic
progress
in Machine Learning that we 
have seen in recent years
is attracting a lot of attention. \textbf{Machine Learning} is
considered  a powerful tool to avoid \textbf{preprogrammed solutions} 
in favor of \textbf{learned solutions}, and 
there are indeed great and fully justified expectations of what machine learning can bring about in relieving 
humans from having to preprogram
mundane adaptation tasks.

However, in machine learning, we typically don't have an undestanding
of how and why a certain solution has been chosen.
%In a sense the system performs its learning and  we hope for the best. 
This \textbf{lack of understandability of machine learning solutions}
is increasingly becoming a concern in both the AI and CS scientific
communities, \cite{RussellDT15,ACMStatement07}, and has been recently
taken up by DARPA, through the DARPA-BAA-16-53 ``Explainable
Artificial Intelligence (XAI)''
program.\footnote{\url{http://www.darpa.mil/program/explainable-artificial-intelligence}}. For
example, the ACM Statement on Algorithmic Transparency and
Accountability \cite{ACMStatement07} says: ``\emph{There is also growing
evidence that some algorithms and analytics can be opaque, making it
impossible to determine when their outputs may be biased or
erroneous.}''

Hence on the one hand
%\begin{quote}\it
there is a widespread 
 recognition in the AI and CS communities that the objectives of
 \project, that is, building 
 self-programming mechanisms that are white-box, are very important.
%\end{quote}
%%
 On the other hand, we are currently stuck between two extremes:
 either we provide preprogrammed solutions, but this is not
 cost-effective or even feasible for certain applications, or we use
 machine learning to produce solutions, but this often means that we
 give up transparency and accountability, and ultimately human
 understandability.
% adopt preprogrammed solutions, but this appears to be not
% cost-effective or even feasible for certain applications; adopt a
% machine learning  solutions, but this may mean to give up
% transparency and accountability, and ultimately human
% understandability. 


Beyond the opaqueness of machine-learning-produced solutions, there
are also widespread concerns about the \textbf{safety} of using such solutions and
whether they are thrustworthy.\footnote{These concerns have been
  widely discussed, see for example
  \url{https://futureoflife.org/background/benefits-risks-of-artificial-intelligence/}. }
For any system that operates autonomously, there must be guarantees
that suitable safety constraints will always be respected. 
% Using formal methods for verification and/or synthesis supports this
% objective.
The relevant safety constraints must be specified in a language that human
users understand.
The system must be able to explain its decisions so that human users
can understand the system's operation and confirm that it does follow
the relevant rules of conduct.
This is crucial for building users'  \textbf{trust} in the system.

\project aims at
addressing the realization of white-box self-programming mechanisms on radically different bases. 
%%
It will leverage on Reasoning about Action and Planning in AI which provide explicite representation, required for being white-box and process synthesis required for self-programming. However traditional Reasoning about Action and Planning are by far too simple to be used off-the-shelf for realizing  white-box self-programming mechanisms. To do so \project will take elements from Verification and Synthesis in Formal Methods and Data-aware processes in Databases, and combine the four areas in a novel way to get the right balance between power and effectiveness.


% \subsection{Self-Programming O1}
\subsection{Generalized Planning} \label{subsec:GP}


%The first ingredient of \project approach is \textbf{Planning}.

\textbf{Planning} is a branch of AI that addresses the problem of generating a
course of action to achieve a specified objective, given an initial
state of the world. It is an area that is central to the development
of intelligent agents and autonomous robots. In the last decades,
automated planning has seen a spectacular progress in terms of
scalability, much of it achieved by the development of heuristic
search approaches as well as symbolic ones  \cite{GeffnerBo13,GNT2016}.

\paragraph{Models.} In Planning, we describe the system by introducing a set of
atomic facts, called fluents, whose truth value changes as the system
evolves, and by specifying preconditions and the effects of actions on
such a set of facts. In each state, corresponding to the truth value
assignment to the fluents, the agent can select which action to
execute among those whose preconditions is satisfied. This way of
modeling comes directly form \textbf{Reasoning about Action} in KR
\cite{Reiter01} and is embedded in the de-facto standard Planning
Domain Definition Language (PDDL) \cite{mcdermott:pddl,gerevini:pddl}
introduced for International Planning Competitions (IPCs).

Two crucial observations are in order. The first is that such modeling
is by construction \textbf{human-comprehensible}, in the sense that
fluents and actions describe the dynamics of the domain of interest in
a high-level terminology ready accessible to humans. This is inherited
by Reasoning about Action and more generally Knowledge Representation
wich is based on an explicit representation of the agent knowledge of
the world it operates in and how it changes through
actions. \textbf{We will retain the human-comprehensibility of the model.}

The second observation is that Planning models make use of
propositional logic (or equivalently first-order logic with a finite
object domain). This makes such models implicite representations of
finite transition systems. Hence these models are readily
\textbf{verifiable}, e.g. by model checking
\cite{ClarkeGP:99-ModelChecking,BaKG08,LomuscioQR17}, which operates
on finite-state systems. \textbf{We will retain the
  verifiability of the model.}
%%
Hence we essentially adopt this modeling features (human-comprehensibility and verifiability) in modeling \project  mechanisms with their
capabilities and the environment we operate. 


However for project \textbf{traditional models used in planning (e.g., PDDL) is too limited to represent mechanisms}. 
%%
So, on the one hand, we will introduce \textbf{rich semantic descriptions} from
Knowledge Representation 
%%
to which the \emph{PI has contributed significantly in the years
\cite{DeGiacomoRS98,DeGiacomoLL00,DeGiacomoLS01,SardinaGLL04,SardinaGLL06,DeGiacomoLP10,DeGiacomoLM12,DeGiacomoLPV14,DeGiacomoLPV16,BanihashemiGL17}}.
In this way we will be able to incorporate data manipulation in
mechanisms, which \textbf{lift models to a to a first-order state
  representation} from a a propositional one. See later.
%%
On the other hand, \textbf{we will incorporate componentization} typical of Service-Oriented Computing \cite{wsf2014,SohrabiPM09,BertoliPT10,DePS13,DeGGPSS16}, so that we can consider the mechanisms formed by coordinated and orchestrated components each with its own features that together form the mechanism with its capabilities. 
\emph{The PI has pioneered work on composition of  stateful-services (services that react depending on the state there are in) \cite{BerardiCGLM03,BerardiCGHM05} and later extended  these results to handle behavior compositions, e.g. of cognitive robotics systems in AI, \cite{SardinaG08,SardinaG09,DePS13,DeGGPSS16}.}




\paragraph{Goals.}
The goal in traditional planning is to find a plan (i.e., a program)
that reach form the initial state a desired state of affairs specified
in terms of a boolean combination of fluents. Notice that again the
\textbf{goal is expressed declaratively at high-level, in a human comprehensible fashion}, as a formula that must
hold in the state resulting from the execution of the plan. \textbf{We will retain this human comprehensibility of the goal}. 
%%
Though for the mechanisms of interest in \project, these
\textbf{traditional form of goals is too limited}. Instead \textbf{we will
handle to full-fledged temporal specification analogous to those used in model
checking and in reactive synthesis} \cite{ClarkeGP:99-ModelChecking,BaKG08,PnRo89}, though sidestepping the notorious difficulties algorithms required for reactive syntesis, by
focusing on non-traditional kinds of specification formalisms proposed
recently in, e.g., Reasoning about Action and Generalized Planning,
such as LTL and LDL on finite traces, recently proposed by the PI
together with Moshe Vardi (Rice U, Huston) \cite{DeVa13,DeVa15,DeVa16}
and adopted in generalized forms of Planning in AI
\cite{TorresB15,CamachoTMBM17} and in declarative business processes
in BPM \cite{AalstPS09,DeGMGMM14,DeGMMP17}, as well as safe/co-safe
LTL/LDL formulas, which have been shown to be more expressive than
expected while remaning \textbf{well-behaved}
\cite{FinkbeinerS13,FiliotJR11,Lacerda0H15,FaymonvilleFRT17}. Solvers
for these are substantially simpler that for general reactive
synthesis, being based on reachability and safety games, which are
amenable to efficient implementations.


\paragraph{Algorithms.}

Planning in its classical version stems from the STRIPS planning model
\cite{FikesN71} that originated at the Shakey project
\cite{Nils-84,KuipersFHN17}.  In classical planning actions are deterministic, that is,
executing an action in a given state will result in a unique resulting
state. Plans of this form are essentially sequence of
actions, each executable when instructed, that lead from the initial
state to a state where the goal is satisfied. In other words, planning
in this context amounts into solving a reachability problem, in a
transition system (finite state machine), whose states are represented
logarithmically through fluents. Indeed the problem is PSPACE
(-complete), being reachability NLOGSPACE and the FSM transition
system exponentially larger then its specification, but constructible
on-the-fly.

% Planning in its classical version stems from the STRIPS planning model
% \cite{FikesN71} that originated at the Shakey project
% \cite{Nils-84,KuipersFHN17}.  In particular, we describe the system by
% introducing a set of atomic facts, called fluents, whose truth value
% changes as the system evolves, and by specifying preconditions and the
% effects of actions on such a set of facts. In each state,
% corresponding to the truth value assignment to the fluents, the agent
% can select which action to execute among those whose preconditions is
% satisfied. In classical planning actions are deterministic, that is,
% executing an action in a given state will result in a unique resulting
% state. The goal in classical planning is to reach form the initial
% state a desired state of affairs specified in terms of a boolean
% combination of fluents. Plans of this form are essentially sequence of
% actions, each executable when instructed, that lead from the initial
% state to a state where the goal is satisfied. In other words, planning
% in this context amounts into solving a reachability problem, in a
% transition system (finite state machine), whose states are represented
% logarithmically through fluents. Indeed the problem is PSPACE
% (-complete), being reachability NLOGSPACE and the FSM transition
% system exponentially larger then its specification, but constructible
% on-the-fly.

%%
  % Nowadays, planning problems themselves are typically expressed in the
  % de-facto standard Planning Domain Definition Language (PDDL)
  % \cite{mcdermott:pddl,gerevini:pddl} introduced for International
  % Planning Competitions (IPCs), which is turn is largely inspired by
  % the work in Knowledge Representation on Reasoning about Action
  % \cite{Reiter01}.


While classical planning assumes deterministic actions, many real-world planning problems are better suited to a model with non-deterministic (in the sense of adversarial) action outcomes. Fully observable non-deterministic (FOND) planning problems require the planner to consider every possible action outcome.  Recent advances have greatly improved the efficiency of FOND planning \cite{KuterNRG08,DMattmullerOHB10,FuNBY11,KissmannE11,MuiseMB12}, especially fo so called strong cyclic planning which guarantees achievement of the goal under an assumption of fairness. 

Planning with sensing actions under partial observability in nondeterministic domains POND has also been studied, being considered fundamental to the realization of AI tasks in areas as diverse as robotics, game playing, and diagnostic problem solving. Indeed plan existence for conditional planning is 2EXPTIME-complete \cite{Rintanen04icaps}. Some early POND planners were based on partial order planning (e.g., CNLP \cite{peot:92}, Cassandra \cite{collins:jair}) and Graphplan (e.g., SGP \cite{anderson:conditional}). MBP \cite{bertoli:mbp} and BBSP \cite{Rintanen04aaai} are more recent BDD-based model checking planners. Planners based on heuristic search include Contingent-FF \cite{HoffmannB06}, ``POND'' \cite{BryceKS06}, and most recently CLG which has an offline variant \cite{AlborePG09,AlboreRG11,BonetG14}.  Experimental results reported in the literature appear to indicate that CLG represents the state of the art in terms of scalability of offline conditional planning.



An agent planning to achieve a goal in a partially observable environment with sensing actions has the option of choosing how to act online by interleaving planning, sensing, and acting.  Online planning is often used  when the domain does not includes so call ``deadends'': i.e., actions whose effects cannot be undone.  In this circumstances online plans are easier to compute since integrating online sensing with planning eliminates the need to plan for a potentially exponential (in the size of relevant unknown facts) number of contingencies. In the absence of deadends, we do have several fast and effective online planners. Recent advances include CLG and CLG+ \cite{AlborePG09,AlboreRG11}, K-Planner \cite{BonetG11,BonetG14}, and SDR \cite{BrafmanS16}.
As deadends are allowed online planing becomes as difficult as offline planning, since even if we are interested only in the first action at each step, to compute it we may need to compute the entire plan exactly to avoid deedends.



One specific observation we can draw is that like agents in Planning,
we expect mechanisms to be able to handle quickly and efficiently most
cases, i.e., those cases that do not require to solve difficult,
``puzzle-like'', situations. Indeed while the Planning community has
concentrated on simpler forms of process synthesis, it has developed a
sort of \textbf{science of search algorithms for Planning}, which has
brought about improvements by orders of magnitude in the last decade
\cite{PommereningHB17,SteinmetzH17,LipovetzkyG17,DeGMMP17}. \project
will exploit this knowledge and extend algorithms and heuristics to
handle generalized forms of Planning and to reactive Synthesis.

Specifically, recent work by the \textbf{PI has explored connections between generalized forms of planning and synthesis} when goals are temporally extended: goals are expressed as requirements on the entire execution instead of the final state of the execution \cite{KabanzaBS97,BacchusK98,CerritoM98,DeGiacomoV99,BacchusK00,CalvaneseGV02}. 
\emph{Promising exploratory results by the PI and other \cite{DeGiacomoFPS10,SardinaD15,DeVa13,DeVa15,CamachoTMBM17}) are available}, but such results are far from clarifying the picture both from a theoretical perspective and from an algorithmic perspective.

Though it appears possible to \textbf{compile} the more general forms of synthesis problems which reduces to reachability and co-reachability/safety games \textbf{into synthetic planning domains so as to exploit the algorithmic insights from Planning}, Classical Planning, FOND and POND, to solve these games. 
%%
In particular,  it appears possible to exploit the correspondence between FOND and 2-player games, and  the significant recent advances in the computational efficiency of FOND planning that have produced FOND planners that scale well in many domains (e.g., NDP \cite{AlfordKNG14}, FIP \cite{FuNBY11,FuJNBY16}, MyND \cite{DMattmullerOHB10}, Gamer \cite{KissmannE11} and PRP \cite{MuiseMB12}). 
% Our insights FOND solvers can be used as a black-box solver for more sophisticated problems (like SAT solvers have been used as tools for solving a variety of more difficult problem, like symbolic bounded model checking). 
Moreover some of these solvers internals can be modified to handle directly reachability and co-reachability/safety games which are at the base of many of the synthesis problems of interest in this project.  
%%
For example, focusing on FOND, most modern algorithms are for strong cyclic planning, instead for reachability games we need to solve strong planning.
This requires some adjustment of the internals of the planner
E.g., in MYND \cite{DMattmullerOHB10}, we need  to move from AO* to LAO* for searching the AND/OR graph, see e.g., ;
in OBDD approaches \cite{TorralbaAKE17}, we need to change the algorithm for growing the "winning" set.
%%
The ``state of the art'' is PRP \cite{MuiseMB12}, which solves strong cyclic planning by  iterated calls to LAMA \cite{RichterW10} and hence uses classical planning algorithms, it's can be adjusted to by adding counters though this would not retain the maximal efficiency. Better techniques need to be developed in this case.

Again the crucial point is that these solvers based on Planning come with very effective domain independent heuristic, which are continuously improved by a scientific community focussing specifically on improving the search process in these solvers. Notice that to get this efficiency we do not need to relay on any specific modeling formalism \cite{FRLG17}.  Hence, in spite of the notable changes in the model and the goals we wil consider in \project, \textbf{we are confident that the planning algorithms will scale up} .





\subsection{Verification and Synthesis}
%The second ingredient of \project approach is \textbf{Reactive Synthesis}.

\project will draw on the rich modeling and algorithmic techniques
from Formal Methods, particularly program verification and synthesis.

The main approach to program verification is \emph{model-checking}.
This is a set of algorithms and techniques, often based on logic and
automata, for automatically determining whether a mathematical model
of a system satisfies a specification formally expressed in
mathematical logic~\cite{ClarkeGP:99-ModelChecking,BaKG08}.  This
field, for which the founders and proponents won two different Turing
awards, has been used successfully for complex systems, and many
hardware and software companies use these methods in practice, e.g.,
verification of VLSI circuits, communication protocols, software
device drivers, real-time embedded systems, and security algorithms. 

The same community has been developing \emph{synthesis}, i.e.,
techniques to automatically construct a system that satisfies a given
specification. Of particular relevance is synthesis applied to
\emph{reactive systems}: those maintain an ongoing interaction with an
external environment.\footnote{This is in contrast to classical
non-reactive programs that transform static inputs to static outputs.}
Such \emph{Reactive Synthesis} is best viewed as a game between the
uncontrollable environment and the program to be synthesized. A
correct program can then be viewed as a winning strategy in this game,
and thus synthesis reduces to finding such a
strategy~\cite{Chu63,PnRo89}. Over the years, the formal-methods
community has exploited this game-theoretic viewpoint and developed a
\textbf{comprehensive and mathematically elegant theory of reactive
synthesis} connecting logics, automata theory and games
\cite{Vard95,KuVa97,KupfermanV05,KupfermanPV06,Vardi08,RyanTacas05,ChatterjeeH07,BloemJPPS12,BohyBFJR12,FinkbeinerS13,LustigV13,FogartyKVW15,DGenestPS15,FearnleyPS15,BloemCJK15,AlurMT16,EsparzaKRS17,BrenguierRS17,HunterPR17}.

In spite of the rich theory developed for program synthesis, little of
this theory has been reduced to practice. 
% In fact, the main approaches to tackle synthesis are either to use
% heuristic approaches \cite{RyanTacas05} or to restrict the kind of
% allowed specification \cite{BloemJPPS12}.
%%
Some researchers argue that this is because the realizability problem
for linear-temporal logic (LTL) specifications is 2EXPTIME-complete
\cite{PnRo89,Rosner92}. However, this argument is not compelling.
First, experience with verification shows that even nonelementary
algorithms can be practical, since the worst-case complexity does not
arise often (cf., the model-checking tool MONA \cite{ElgaardKM98}).
Furthermore, in some sense, synthesis is not harder than verification.
This may seem to contradict the known fact that while verification is
linear in the size of the model and at most exponential in the size of
the specification \cite{ClarkeGP:99-ModelChecking}, synthesis from LTL
specifications is 2EXPTIME-complete. There is, however, something
misleading in this fact: while the complexity of synthesis is given
with respect to the specification only, the complexity of verification
is given with respect to the specification and the program, which can
be much larger than the specification. In particular, it is shown in
\cite{Rosner92} that there are temporal specifications for which every
realizing program must be at least doubly exponentially larger than
the specifications. Clearly, the verification of such programs is
doubly exponential in the specification, just as the cost of
synthesis.


We believe there are a number of reasons for the \textbf{lack of
practical impact of the theory of reactive synthesis}.  We list 4 of
these, and include principles and directions for overcoming them.


(i) \textbf{The focus on expressing both the specification and the
model in LTL (as in reactive synthesis) hides the relevant
complexities}.

Exactly as in model checking, we need to distinguish the
\textbf{complexity wrt the model of the mechanisms} and the
\textbf{complexity wrt the specification of its tasks (goals)}. In
this light, the complexity of solving games with LTL objectives is
PTIME-complete in the size of the model, i.e., EXPTIME-complete in the
compact representation of the model, and 2EXPTIME-complete in the size
of the specification formula of the goal. Thus, even in the case that
the model is given compactly (as in Planning), the complexity wrt the
model is much lower than the complexity wrt the specification. This is
a particularly important observation, since in general the model is
going to be much larger than the goals.

(ii) \textbf{The specifications languages lead to constructions that
are too complex}.  The approach to LTL synthesis or solving LTL games
involves two main steps. First, constructing parity tree-automata that
realize all winning strategies, and second, testing such automata for
emptiness. The first step can be done using determinization of B\"uchi
automata. Unfortunately, determinization of these automata is not as
simple as determinization of ordinary automata (on finite strings),
and has been notoriously resistant to efficient implementations
\cite{TsaiFVT14}.  Alternative constructions that avoid
determinization~\cite{KupfermanV05,KupfermanPV06} did not prove to be
radically more efficient~\cite{FiliotJR11}. The second step involves
solving parity games. This problem, solvable in quasi-polynomial time,
is not known to be in PTIME~\cite{DBLP:conf/stoc/CaludeJKL017} and has
been attacked with many different algorithms.\footnote{See
\url{https://github.com/tcsprojects/pgsolver}.}

% The best-known algorithms for parity-tree-automata emptiness
% \cite{JurdzinskiPZ08} are nontrivial already when applied to simple
% state spaces. Implementing them on top of the messy state space that
% results from the reduction form LTL is awfully complex, and is not
% amenable to optimizations and a symbolic implementation.

% 
% !!!!!!!!!!!!!!!!!!!
% An interesting approach is that of bounded synthesis
% \cite{FinkbeinerS13,FiliotJR11,FaymonvilleFRT17} in which the LTL
% formula is first transformed into a coB\"uchi automaton on trees
% which is then further reduced a safety game given the maximum size of
% the controller to be synthesized. Then iteratively the size in
% incremented until strategy is found or an exponential limit is reached
% which guarantees that no strategy will be found at all. This
% incrementally nature is quite interesting (see below), but
% construction still too complex for being applied in real cases
% \cite{JacobsBBK0KKLNP16}.
% !!!!!!!!!!!!!!!!!!!

However, there are relevant classes of specifications, coming from
generalized planning in AI and declarative business processes,
\textbf{that sidestep these difficulties alltogether}.  These advocate
the use of linear-temporal logics over \emph{finite traces}, i.e.,
LTL$_f$ and its MSO-complete extension LDL$_f$. The principle
technical advantage of using these is that they involve
\emph{ordinary} automata on finite words which are indeed amenable to
quite good implementations
\cite{AalstPS09,DeVa13,DeVa15,DeVa16,TorresB15,CamachoTMBM17}.



(iii) \textbf{Techniques are optimized for solving difficult synthesis
problems, including puzzle-like ones}.
%%
Reactive synthesis is typically applied to low-level problems in which
there is no reason to think that solutions are going to be easy to
find --- the planning literature calls these puzzle-like problems.  On
the other hand, it is unrealistic to require \project to handle such
artificial problems.  Instead, just as in Planning, \project will aim
at solving large problems that are not puzzle-like.  Following this
assumption, Planning has obtained a spectacular improvement in the
last two decades, relying on heuristics which ultimately can be seen
as abstraction that relax detail of the problem at hand. In \project
we will apply such heuristics to synthesis.  We will also explore
promising techniques from the formal-methods community, i.e.,
bounded-synthesis where small controllers are explored before larger
ones~\cite{FinkbeinerS13,FiliotJR11,FaymonvilleFRT17} (currently, even
this construction is too complicated to be applied in real
cases~\cite{JacobsBBK0KKLNP16}).  {\color{red}{Are you sure?}}

% since 
% the domains of interest for applying \project will not 
% be artificial puzzle-like problems. and will perform synthesis there. Moreover, the
% synthesis will be performed while the mechanism are in execution. SO
% it is unrealistic that to require such mechanism to handle ``puzzle
% like settings'', as called in the Planning literature. Instead exactly
% like advocated in Planning they should aim at easy solutions first,
% which probably will be available indeed. Following this assumption
% Planning has obtained a spectacular improvement in the last two
% decades, relying on heuristics which ultimately can be seen as
% abstraction that relax detail of the problem at hand. This can be done
% also in synthesis and will be explored in \project.
% 
% Moreover, with the exception of bounded synthesis, 
% algorithms do not try to find for easy
% solution first (possibly with the exception of bounded synthesis,
% where smaller controllers are explored before larger ones).  


(iv)  \textbf{Most synthesis techniques are not compositional}.
%%
  This is a major methodological issue. Most current theory of program
  synthesis assumes that one gets a comprehensive set of temporal
  assertions as a starting point. In the context of \project and the
  applications we have in mind, this is not realistic.  A more
  realistic approach would be to assume an evolving formal
  specification: temporal assertions can be added, deleted, or
  modified. Since it is rare to have a complete set of assertions at
  the very start of the design process, there is a need to develop
  compositional synthesis algorithms. Such algorithms can, for
  example, refine designs when provided with additional temporal
  properties \cite{KupfermanPV06,FiliotJR11,AlurMT16}.
  {\color{red}{Are these the right citations?}} The idea of composing
  solutions has been pioneered in service-oriented computing, and
  indeed the work by the PI pioneered this approach
  \cite{BerardiCGLM03,DeGiacomoFPS10,DePS13,CalvaneseGLV16}.

\subsection{KR and First-Order State Representation}

The third ingredient of \project approach is \textbf{Knowledge Representation} and \textbf{Reasoning about Action}.


The key property of white-box self-programming mechanisms is the
ability of describing their specification, the programs they generate
and the relationship between the two in human terms.

To do so the domain where the mechanism operates, the 
mechanism itself, its capabilities and limitations, as well as
its specifications and goals need to be formally described in terms of concepts that
can be shared with humans.

\project will consider this issue upfront, by founding white-box self-programming mechanisms on the area of AI called \textbf{Knowledge Representation}. 

\emph{
The PI is  a prominent member of the scientific community working on Knowledge Representation. He is indeed the single researcher with most papers appeared in the Flagship Conference of the community: the International Conference on Principles of Knowledge Representation and Reasoning (KR) ranked A* according to CORE.
}


\textbf{Knowledge Representation} stems from a deep
tradition in logic. In particular, it aims at building systems that
know about their world they operate in and are able to act in an informed way in it,
as humans do.
%%
A crucial part of these systems is that knowledge is represented
``\emph{symbolically}'', and that ``\emph{reasoning procedures}'' are able to extract
consequences of such knowledge as new symbolic representations. Such
an ability is used to deliberate in an informed fashion the course of
actions to take, understanding the consequences of the action performed.

In \cite{Levesque14,Levesque17}, it is argued that Knowledge Representation is
radically new idea in human history. It comes about after a long
gestation, stemming from Aristotle, who developed the initial notion
of logic though unrelated to notion of computation; continued by
Leibniz, who brought forward a notion of ``thinking as computation'',
though not yet symbolic; and later by Frege, who developed the notion
of symbolic logic, though unrelated to computation; and finally by the
breakthrough in human thinking of the early part of last century with
Church, Godel, and Turing, who set the bases for symbolic logic bound
together with computation and ultimately for Computer Science, though
even them did not think about logic as a way of representing
knowledge. The use of a symbolic systems to represent knowledge
expressed in terms of human concepts can only be traced back to
McCarthy's 1959 seminal work on Artificial Intelligence \cite{McCa57}.

Knowledge Representation has developed enormously since then in
diverse directions and subareas. Here we focus mainly on the area of reasoning about action.



\textbf{Reasoning about Action} takes a first-person view of an agent. %
\footnote{This contrast with the work in Multi Agents Systems, where
  typically a third person view (a ``designer'' view) is adopted.} %
The agent as a representation of the domain in which it operates;
a  representation of 
 the possible actions in terms of preconditions and effects formally expressed over the representation of domain;
finally it has a representation of  complex agent behaviors and capabilities typically  described through high-level programs, whose atomic instruction and tests corresponds, respectively, to actions and queries over the representation of the domain.
%%
Through 
reasoning on these representations the agent undestand what doing an action or enacting a certain behavior will bring about, enabling it to make informed decisions on which action to choose or to exclude in relation to its current tasks/goals/specifications.
%%
Reasoning about action has been studied in depth through
comprehensive frameworks, such as that of Situation Calculus \cite{McHa69,Reiter01}. 
As mentioned, it has deeply influenced the models used in Planning, including PDDL, which, though drastically simplified, come directly from these studies.


\emph{The PI has deeply contributed to the development of Reasoning about
Action. In particular, within the framework of Sitation Calculus. Specifically, the PI
introduced of the high-level programming languages
ConGolog and IndiGolog, the
distinction between off-line executions and online executions, the
notion of execution monitoring and recovery, interleaving execution
and planning, the notion of ability \cite{DeGiacomoRS98,DeGiacomoLL00,SardinaGLL04,SardinaGLL06}.}

It is important ot notice that the work Reasoning about Action in
Situation Calculus as well as in most other frameworks, is based on a
\textbf{First-Order Representation of the State}. In other words the
current state of the Agent is represented in terms of
predicates/relations.  This give rise to infinite transition systems
which are typically impossible to verify directly.

\emph{Recently the PI has devised a set of novel results
that shows the effective computability of expressive variants of the
original full-fledged (predicate based) Situation Calculus \cite{DeGiacomoLPV14,DeGiacomoLPV16,DeGiacomoLPS16,BanihashemiGL17,CDMP17}. Such results are being complemented by the
possibility of combining action theories with ontological
representations in description logics \cite{CalvaneseGLR07,CalvaneseGLMS12,HaririCMGMF13,CalvaneseGS15}.} Moreover, the techniques for applying belief revision to
transition systems based proposed
recently in \cite{HerzigMBW14,CarrilloR14}, open up the possibility of grounding
computationally the notion of ``model revision'' for mechanisms.

% Moreover, in Reasoning about Action restricted forms of representations
% (essentially propositional vs first-order) have been put forward to study efficient
% action deliberation and planning, which in these years is producing a
% vast array of particularly fruitful results [GeBo13, DePS10, DFPS10,
% CaDH11, FeDL12, DePS13, DeVa13, GeTh14, DeDM14, DDGM14].

% In fact  the standard language PDDL itself universally used for Planning is largely inspired by the research in Reasoning about Action.


We stress that in \project we intend to represents that state of the mechanism as well as the state of the enviroment in with it operates, in a semantically rich fashion. To do so we relay on Description Logic and Ontology-Based Data Access.

\textbf{Description Logics} are the formalism of election in for
representing the information on the domain of interest in terms of
objects grouped in classes or concepts and relationships among such
classes.  Moreover \textbf{description logics} are nowadays considered
the standard formalism for ontologies and deeply shaped semantic
technologies including the current W3C standard OWL 2.

\emph{The PI has deeply contributed in the development of description
logics. First he worked on the correspondence between expressive
description logics and modal logics of programs such as Dynamic Logic \cite{DeGiacomoL96,CalvaneseGL98}.
Then more recently, together with his group in Rome he developed one
of the best know light weight description logics, DL-lite \cite{CalvaneseGLLR07}, which is essentially
able to formalize UML class diagrams and Entity-Relationship diagrams,
while keeping inference and conjunctive query answering tractable (the
latter in AC0, the same cost as standard SQL). DL-lite in turn made it possible to develop \textbf{Ontology Based Data Access}, which can be considered the most successful use of semantic technologie for data integration \cite{PoggiLCGLR08,SequedaM17,Statoil17}. }

The role of these technologies in \project is to represent in terms of an ontology chosen so has to capture the doman on interest in which the mechanism is operating as well as the (high-level) data structures of the mechanism itself in terms that are sharable by humans as e.g., in \cite{TenorthB17}. In particular \project intends to exploiting the techniques for efficient query-aswering provided by DL-lite and Ontology Based Data Access, especially in the write-also variants, explored recently \cite{DORS16,DLOST17}.


Notice that on the other hand \project does not aim at defining new
concrete representation languages. Instead it intends to use
well-known formalisms such as BPMN, UML, OWL, etc. as concrete
languages, though with a precise formal semantics to allow for
automated reasoning, verification and synthesis, see e.g.,
\cite{BerardiCG05,DeGiacomoOET17}.

%\textbf {Excellent representation formalisms but limited algorithms.}



\subsection{Data-Awareness}

% The fourth ingredient of \project approach is \textbf{Data-aware processes} modeling and analysis.

Crucially, differently from current work in Planning and in
Verification and Synthesis, which operate with a propositional
representation of the state, \project does not want to discard data, and hence
it will need to consider a first-order or relational representation of
the state. This gives rise to infinite transition systems which are generally problematic to analyze.
%%
\emph{However the PI
has already shown within the EU FP7-ICT-257593 ACSI: Artifact-Centric
Service Interoperation, that such difficulties can be overcome in
notable cases
\cite{BerardiCGHM05,CalvaneseGHS09,HaririCGDM13,CalvaneseGMP13} in the context of \textbf{Data-Aware Processes} in Databases.} A key
decovery is that under natural assumptions these infinite-state
transitions systems admit faithful \textbf{abstractions} to
finite-state ones, hence enabling the possibility of using the large
body of techniques developed within  Verification and Synthesis in
Formal Methods. Moreover important advancements in undestanding how to
deal with such complexity have been established as well as
relationships with Reasoning about Action in KR
\cite{HaririCMGMF13,BelardinelliLP14,CalvaneseGS15,CDMP17,BanihashemiGL17}.
We will leverage on these ideas to lift our results so
as to handle data.

\textbf{Excellent results, but no synthesis.}

\subsection{Componentization}
\project intends to \textbf{support component-based approaches.} 


Interfaces capture essential properties of components while hiding
irrelevant details. Interface theories \cite{AlfaroH01} provide
composition operators, compatibility notions (is the composition of a
set of components legal?), and conditions for substitutability (when
can one component be replaced by another without compromising the
properties of the overall system?) that enable component-based design
and alleviate state explosion. Compositionality and incrementality
raise a number of questions in the context of synthesis: How to
separately synthesize controllers for individual components, or for
the same component but with respect to different properties, and then
combine them into a single controller? How to derive the component
interfaces? Because components take different forms depending on the
application domain (they can be pieces of hardware, of software, or of
models written in a high-level language such as Simulink or UML),
there is no unique, “one-size-fits-all” interface model. For instance,
interfaces carry different information in synchronous vs. dataflow
components, or when reasoning about I/O dependencies vs. correctness
vs. timing and performance properties \cite{LublinermanT08,LublinermanST09,TripakisLHL11}. Domain knowledge is key in
identifying the right level of abstraction and information content of
the interfaces. In conjunction with this, we will develop methods to
derive composite interfaces from basic interfaces automatically, thus
maximizing the synergy of human and synthesizer.

\subsection{Integrating stochastic decision and reinforcement learning} 

\project aims at \textbf{allowing learning and stochastic decisions, while remaining within safe bounds}.


While classical formal verification and synthesis relies on deductive reasoning and decision procedures for constraint-satisfaction problems, our view of synthesis also involves inference of program components from example behaviors, both positive and negative, using computational learning. While there is a wealth of literature on learning theory (see, for instance, \cite{KeVa94}), we plan to explore how it can be adopted and advanced in the context of program synthesis. As an illustrative example, consider the recent interactive add-in for Microsoft Excel spreadsheet software based on the concept of ``programming by examples'' \cite{Gulwani11,Gulwani16}. In this system, the user demonstrates the desired transformation task (for example, rewriting all names to a uniform format such as first-name followed by a single space, followed by last-name) using a few examples, and the synthesis tool automatically generates the “best” program (in the form of an Excel Macro) that is consistent with all the examples. The synthesized program is then used to transform all the entries in the spreadsheet, and if this synthesized transformation does not match the user’s intent, the user can guide the synthesizer by highlighting the incorrect updates, thus, providing negative examples for the subsequent iteration. 
%%
While this tool  has the potential of enormous impact by facilitating intuitive programming by millions of end users, the theoretical foundations of learning the desired program from examples are not yet understood. 
%%
A suitable theoretical abstraction for the desired program is a ``string-to-string'' transducer. While there is a well developed theory of minimization and learning for the class of sequential transducers \cite{Mohri00}, sequential transducers are not expressive enough to capture the typical transformations that involve swapping of substrings. The recently proposed model of streaming string transducers \cite{AlurC11} has appealing theoretical properties, such as well characterized expressiveness. Since this model can capture the desired Excel transformations, algorithms for minimization and learning—a topic for proposed research, can lead to efficient synthesis procedure with guarantees of convergence.


\subsection{Driving Application Contexts}

\project will ground its scientific results in three
diverse real \textbf{application} to demonstrate the actual utilization of the scientific
achievements within the project: Smart manufacturing (Industry 4.0),
 Smart spaces (IoT), and
Business Processes Management Systems (BPM).
%%
\emph{The PI and his group at Sapienza have
contributed to all these fields, see e.g.,
\cite{DeGiacomoCFHM12,DeGiacomoDMM15,SilvaFCLSR17}.  Moreover the PI
has applied advanced science to real-cases in the
area Semantic Data Integration where he and his group have invented the Ontology-Based Data Access paradigm, possibly the
most successful approach for Semantic Data Integration \cite{PoggiLCGLR08,SequedaM17,Statoil17}.
%he contributed to W3C recommendation of OWL 2 Web Ontology Language
%Profiles (\url{https://www.w3.org/TR/owl2-profiles/}); 
Such an approach has matured to the point that  the PI
founded a Sapienza Start-Up \textbf{OBDA Systems}
(\url{http://www.obdasystems.com}) to commercially exploit it in real data integration scenarios. }


\subsubsection{Smart Manufacturing}

\subsubsection{Internet of Things}

\subsubsection{Business Processes}
% 
% \subsection{OBJECTIVES}

% \vspace{-1ex}

% The overarching objective of \project is the following:

% \begin{quote}\textit{
% \project aims at laying the theoretical foundations and practical
% methodologies of a science and engineering of \textbf{white-box self-programming mechanisms}. 
% }
% \end{quote}
% \project intends to consider self-programmable mechanisms, as forms
% of \textbf{Agents} studied in \textbf{Artificial Intelligence}
% \cite{Reiter01,Wooldridge09}. \footnote{We stress that \project does
%   not aim at general AI, but envisions self-programming mechanisms
%   that act intelligently within the specific domain of interest in
%   which they operate.}

% Since ``\emph{with great power comes great responsibility}'',
% introducing advanced forms of self-programming calls for the ability
% to make the behavior automatically synthesized by the mechanism
% \textbf{understandable} to human supervisors.
% %%
% %must be checked to be \textbf{harmless}. 
% % In line with a large part of the AI community, 
% % \project considers this point essential \cite{RussellDT15}.
% %%
% So it is indeed crucial to develop self-programming mechanisms that are \textbf{white-box}: in every moment the
% mechanism can be queried for its specifications, its behavior and how
% it relates to the specifications. Ultimately it is the possibility of \textbf{explain in human terms}
% the resulting behavior that will make white-box self-programming mechanisms  \textbf{trustworthy} \cite{CaDa10,Neumann17}.
% %
% Being white-box contrasts with most current approaches, which consider
% acceptable synthesized solutions that remain opaque to humans, as long
% as they work \cite{MnihKSGAWR13,SilverHMGSDSAPL16}.

% We further stress that the need to move towards \textbf{white-box}
% approaches is advocated by a large part of the \textbf{AI community}
% \cite{RussellDT15}, and has been recently taken up by DARPA within the
% context of machine learning, through the DARPA-BAA-16-53 ``Explainable
% Artificial Intelligence (XAI)''
% program\footnote{\url{http://www.darpa.mil/program/explainable-artificial-intelligence}}

% \begin{quote}{\it
% Knowledge representation, the primary field of the PI, will be central for realizing the shift towards a white-box approach.}
% \end{quote}
% %\vspace{-1ex}



% Towards the goal of building \textbf{white-box self-programming mechanisms}, \project will address the following objectives. %challenges.
% \begin{enumerate}

% \item \textbf{Equip mechanisms with general self-programming abilities.}

% \item \textbf{Make self-programming abilities available while in operation.} 

% \item \textbf{Make white-box self-programming mechanisms verifiable.}

% \item 
% \textbf{Allow learning and stochastic decisions, while remaining within safe bounds.}

% \item \textbf{Make white-box self-programming mechanisms comprehensible to
%     humans}. 
% \item \textbf{Make self-programming mechanisms data-aware.}

% \item \textbf{Favor component-based approaches.}  
% \end{enumerate}


\subsection{High Risk, High Gain}

Recent foundational results by the PI chart a novel path that within \project will revolutionize \textbf{Reasoning about Action}  in KR and \textbf{Generalized Planning} in AI by introducing rich objectives, data, and componentization in order to produce a \textbf{breakthrough in engineering self-programming mechanisms that are human-comprehensible and safe by design.}
%%
Moreover \project aims at being the spark that will bring together and
cross-fertilize four distinct research areas, namely \textbf{Reasoning
  about Action} in \emph{Knowledge Representation}, \textbf{Data-aware
  Processes} in \emph{Databases}, \textbf{Verification and Synthesis}
in \emph{Formal Methods}, and \textbf{Generalized Planning} in
\emph{AI}.
%%
The PI has profoundly contributed to all these areas, and he is %in a unique position to thrive this cross-fertilization.
one of the most prominent AI scientist leading this cross-fertilization.




\project is a \textbf{high risk, high gain project}.
%%
It is \textbf{high gain} since, if successful, it
will result in a radically more useful automated mechanisms than what
we have today, namely \textbf{white-box self-programming mechanisms},
unleashing full potential of \textbf{self-programmability} and
removing the main barriers to the uptake of automated mechanisms in
real business context, namely \textit{predefined forms of automation}, and
difficulties in \textit{formally analyzing their automated behavior in human terms}.
%
Given the crucial role that automated mechanisms plays in our modern economy and society and that \textbf{white-box self-programming mechanisms} have the potential to resolve a number of central hurdles, % (see part B1 for both), 
this implies a potentially very high impact on computer science as well as in crucial  business contexts, facilitating the already ongoing uptake of automated mechanisms in industry eventually also on economy and society. 
%
As a result, \project is \textbf{very timely} and of \textbf{greatest significance for European science}.




The \project  involves \textbf{high risk} because the ultimately we need to merge explicit representation, with advanced form of process synthesis/coordination and refinement and  identify novel and useful islands of effective feasibility that \project aims at is technically extremely challenging, much more so than the related Verification and  Planning both of which have made tremendous progresses in the last decade, touching upon several notorious open problems in computer science. 
%%
On top of that, the path from theoretical analysis to practically efficient algorithm, which we plan to fully explore within \project, is a huge challenge given that we aim realizing real automated mechanisms to be deployed in real business scenarios. 

Despite the challenges that \project will face, as discussed above, the general \textbf{feasibility} of \project is demonstrated by the PI original, exploratory publications \cite{DeVa13,DeVa15,DeVa16,DeGMGMM14,DePS13,DeGGPSS16}, and follow ups \cite{TorresB15,CamachoTMBM17}. 


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "PartB2"
%%% TeX-PDF-mode: t
%%% End:
